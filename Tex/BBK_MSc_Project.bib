Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{KenjiIwana,
abstract = {Book covers communicate information to potential readers, but can that same information be learned by computers? We propose using a deep Convolutional Neural Network (CNN) to predict the genre of a book based on the visual clues provided by its cover. The purpose of this research is to investigate whether relationships between books and their covers can be learned. However, determining the genre of a book is a difficult task because covers can be ambiguous and genres can be overarching. Despite this, we show that a CNN can extract features and learn underlying design rules set by the designer to define a genre. Using machine learning, we can bring the large amount of resources available to the book cover design process. In addition, we present a new challenging dataset that can be used for many pattern recognition tasks.},
archivePrefix = {arXiv},
arxivId = {1610.09204},
author = {Iwana, Brian Kenji and Rizvi, Syed Tahseen Raza and Ahmed, Sheraz and Dengel, Andreas and Uchida, Seiichi},
doi = {10.1080/10464883.2018.1412204},
eprint = {1610.09204},
file = {:C$\backslash$:/Users/rlhil/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kenji Iwana et al. - Unknown - Judging a Book by its Cover.pdf:pdf},
issn = {1531314X},
journal = {Journal of Architectural Education},
keywords = {Atelier Bow-Wow,Da-me Architecture,Everyday Urbanism,Tokyo},
month = {oct},
number = {1},
pages = {180--181},
title = {{Judging a Book By its Cover}},
url = {http://arxiv.org/abs/1610.09204},
volume = {72},
year = {2016}
}
@article{Olah2017,
abstract = {How neural networks build up their understanding of images.},
author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
doi = {10.23915/distill.00007},
file = {:C$\backslash$:/Users/rlhil/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Olah, Mordvintsev, Schubert - 2017 - Feature Visualization.pdf:pdf},
issn = {2476-0757},
journal = {Distill},
month = {nov},
number = {11},
pages = {e7},
publisher = {Distill Working Group},
title = {{Feature Visualization}},
url = {https://distill.pub/2017/feature-visualization},
volume = {2},
year = {2017}
}
@article{Papert1966,
author = {Papert, Seymour A.},
journal = {MIT Home},
month = {jul},
title = {{The Summer Vision Project}},
year = {1966}
}
@article{Zhuang2019,
abstract = {Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. As the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning researches, as well as to summarize and interpret the mechanisms and the strategies in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Different from previous surveys, this survey paper reviews over forty representative transfer learning approaches from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-21578, and Office-31. And the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.},
archivePrefix = {arXiv},
arxivId = {1911.02685},
author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
eprint = {1911.02685},
file = {:C$\backslash$:/Users/rlhil/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhuang et al. - 2019 - A Comprehensive Survey on Transfer Learning.pdf:pdf},
isbn = {1911.02685v2},
keywords = {Index Terms-Transfer learning,domain adaptation,interpretation ✦,machine learning},
month = {nov},
title = {{A Comprehensive Survey on Transfer Learning}},
url = {http://arxiv.org/abs/1911.02685},
year = {2019}
}
@article{Maity2018,
abstract = {A book's success/popularity depends on various parameters - extrinsic and intrinsic. In this paper, we study how the book reading characteristics might influence the popularity of a book. Towards this objective, we perform a cross-platform study of Goodreads entities and attempt to establish the connection between various Goodreads entities and the popular books ("Amazon best sellers"). We analyze the collective reading behavior on Goodreads platform and quantify various characteristic features of the Goodreads entities to identify differences between these Amazon best sellers (ABS) and the other non-best selling books. We then develop a prediction model using the characteristic features to predict if a book shall become a best seller after one month (15 days) since its publication. On a balanced set, we are able to achieve a very high average accuracy of 88.72{\%} (85.66{\%}) for the prediction where the other competitive class contains books which are randomly selected from the Goodreads dataset. Our method primarily based on features derived from user posts and genre related characteristic properties achieves an improvement of 16.4{\%} over the traditional popularity factors (ratings, reviews) based baseline methods. We also evaluate our model with two more competitive set of books a) that are both highly rated and have received a large number of reviews (but are not best sellers) (HRHR) and b) Goodreads Choice Awards Nominated books which are non-best sellers (GCAN). We are able to achieve quite good results with very high average accuracy of 87.1{\%} and as well a high ROC for ABS vs GCAN. For ABS vs HRHR, our model yields a high average accuracy of 86.22{\%}.},
archivePrefix = {arXiv},
arxivId = {1809.07354},
author = {Maity, Suman Kalyan and Panigrahi, Abhishek and Mukherjee, Animesh},
doi = {10.1007/978-3-030-02592-2_11},
eprint = {1809.07354},
file = {:C$\backslash$:/Users/rlhil/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maity, Panigrahi, Mukherjee - 2018 - Analyzing Social Book Reading Behavior on Goodreads and how it predicts Amazon Best Sellers.pdf:pdf},
month = {sep},
pages = {211--235},
title = {{Analyzing Social Book Reading Behavior on Goodreads and how it predicts Amazon Best Sellers}},
url = {http://arxiv.org/abs/1809.07354},
year = {2018}
}
@article{Cohen,
abstract = {The MNIST dataset has become a standard benchmark for learning, classification and computer vision systems. Contributing to its widespread adoption are the understandable and intuitive nature of the task, its relatively small size and storage requirements and the accessibility and ease-of-use of the database itself. The MNIST database was derived from a larger dataset known as the NIST Special Database 19 which contains digits, uppercase and lowercase handwritten letters. This paper introduces a variant of the full NIST dataset, which we have called Extended MNIST (EMNIST), which follows the same conversion paradigm used to create the MNIST dataset. The result is a set of datasets that constitute a more challenging classification tasks involving letters and digits, and that shares the same image structure and parameters as the original MNIST task, allowing for direct compatibility with all existing classifiers and systems. Benchmark results are presented along with a validation of the conversion process through the comparison of the classification results on converted NIST digits and the MNIST digits.},
annote = {Can use this as a way to resample the book cover images in such a way that they are not distorted?},
archivePrefix = {arXiv},
arxivId = {1702.05373},
author = {Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and van Schaik, Andr{\'{e}}},
eprint = {1702.05373},
file = {:C$\backslash$:/Users/rlhil/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cohen et al. - Unknown - EMNIST an extension of MNIST to handwritten letters.pdf:pdf},
month = {feb},
title = {{EMNIST: an extension of MNIST to handwritten letters}},
url = {http://arxiv.org/abs/1702.05373},
year = {2017}
}
@article{Cohena,
abstract = {The MNIST dataset has become a standard benchmark for learning, classification and computer vision systems. Contributing to its widespread adoption are the understandable and intuitive nature of the task, its relatively small size and storage requirements and the accessibility and ease-of-use of the database itself. The MNIST database was derived from a larger dataset known as the NIST Special Database 19 which contains digits, uppercase and lowercase handwritten letters. This paper introduces a variant of the full NIST dataset, which we have called Extended MNIST (EMNIST), which follows the same conversion paradigm used to create the MNIST dataset. The result is a set of datasets that constitute a more challenging classification tasks involving letters and digits, and that shares the same image structure and parameters as the original MNIST task, allowing for direct compatibility with all existing classifiers and systems. Benchmark results are presented along with a validation of the conversion process through the comparison of the classification results on converted NIST digits and the MNIST digits.},
archivePrefix = {arXiv},
arxivId = {1702.05373},
author = {Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and van Schaik, Andr{\'{e}}},
eprint = {1702.05373},
file = {:C$\backslash$:/Users/rlhil/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cohen et al. - Unknown - EMNIST an extension of MNIST to handwritten letters(2).pdf:pdf},
month = {feb},
title = {{EMNIST: an extension of MNIST to handwritten letters}},
url = {http://arxiv.org/abs/1702.05373},
year = {2017}
}
@misc{Karim2019,
abstract = {A compiled visualisation of the common convolutional neural networks.},
author = {Karim, Raimi},
booktitle = {towardsdatascience},
title = {{Illustrated: 10 CNN Architectures - Towards Data Science}},
url = {https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d},
urldate = {2020-01-23},
year = {2019}
}
@article{Karayev2013,
abstract = {The style of an image plays a significant role in how it is viewed, but style has received little attention in computer vision research. We describe an approach to predicting style of images, and perform a thorough evaluation of different image features for these tasks. We find that features learned in a multi-layer network generally perform best -- even when trained with object class (not style) labels. Our large-scale learning methods results in the best published performance on an existing dataset of aesthetic ratings and photographic style annotations. We present two novel datasets: 80K Flickr photographs annotated with 20 curated style labels, and 85K paintings annotated with 25 style/genre labels. Our approach shows excellent classification performance on both datasets. We use the learned classifiers to extend traditional tag-based image search to consider stylistic constraints, and demonstrate cross-dataset understanding of style.},
archivePrefix = {arXiv},
arxivId = {1311.3715},
author = {Karayev, Sergey and Trentacoste, Matthew and Han, Helen and Agarwala, Aseem and Darrell, Trevor and Hertzmann, Aaron and Winnemoeller, Holger},
eprint = {1311.3715},
file = {:C$\backslash$:/Users/rlhil/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karayev et al. - 2013 - Recognizing Image Style.pdf:pdf},
journal = {BMVC 2014 - Proceedings of the British Machine Vision Conference 2014},
month = {nov},
publisher = {British Machine Vision Association, BMVA},
title = {{Recognizing Image Style}},
url = {http://arxiv.org/abs/1311.3715},
year = {2013}
}
@article{Krizhevsky2017,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%}, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used nonsaturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
doi = {10.1145/3065386},
file = {:C$\backslash$:/Users/rlhil/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2017 - ImageNet classification with deep convolutional neural networks.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
month = {may},
number = {6},
pages = {84--90},
publisher = {Association for Computing Machinery},
title = {{ImageNet classification with deep convolutional neural networks}},
url = {http://dl.acm.org/citation.cfm?doid=3098997.3065386},
volume = {60},
year = {2017}
}
@article{Hubel1959,
author = {Hubel, D. H. and Wiesel, T. N.},
doi = {10.1113/jphysiol.1959.sp006308},
file = {:C$\backslash$:/Users/rlhil/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hubel, Wiesel - 1959 - Receptive fields of single neurones in the cat's striate cortex(2).pdf:pdf},
issn = {00223751},
journal = {The Journal of Physiology},
month = {oct},
number = {3},
pages = {574--591},
pmid = {14403679},
publisher = {Wiley-Blackwell},
title = {{Receptive fields of single neurones in the cat's striate cortex}},
url = {http://doi.wiley.com/10.1113/jphysiol.1959.sp006308},
volume = {148},
year = {1959}
}
@misc{Demush,
author = {Demush, Rostyslav},
booktitle = {Hackernoon},
title = {{A Brief History of Computer Vision (and Convolutional Neural Networks)}},
url = {https://hackernoon.com/a-brief-history-of-computer-vision-and-convolutional-neural-networks-8fe8aacc79f3},
urldate = {2020-03-20},
year = {2019}
}
@article{Qin2018a,
abstract = {Nowadays, the Convolutional Neural Networks (CNNs) have achieved impressive performance on many computer vision related tasks, such as object detection, image recognition, image retrieval, etc. These achievements benefit from the CNNs outstanding capability to learn the input features with deep layers of neuron structures and iterative training process. However, these learned features are hard to identify and interpret from a human vision perspective, causing a lack of understanding of the CNNs internal working mechanism. To improve the CNN interpretability, the CNN visualization is well utilized as a qualitative analysis method, which translates the internal features into visually perceptible patterns. And many CNN visualization works have been proposed in the literature to interpret the CNN in perspectives of network structure, operation, and semantic concept. In this paper, we expect to provide a comprehensive survey of several representative CNN visualization methods, including Activation Maximization, Network Inversion, Deconvolutional Neural Networks (DeconvNet), and Network Dissection based visualization. These methods are presented in terms of motivations, algorithms, and experiment results. Based on these visualization methods, we also discuss their practical applications to demonstrate the significance of the CNN interpretability in areas of network design, optimization, security enhancement, etc.},
archivePrefix = {arXiv},
arxivId = {1804.11191},
author = {Qin, Zhuwei and Yu, Fuxun and Liu, Chenchen and Chen, Xiang},
eprint = {1804.11191},
journal = {Mathematical Foundations of Computing},
month = {apr},
number = {2},
pages = {149--180},
publisher = {American Institute of Mathematical Sciences (AIMS)},
title = {{How convolutional neural network see the world - A survey of convolutional neural network visualization methods}},
url = {http://arxiv.org/abs/1804.11191},
volume = {1},
year = {2018}
}
@inproceedings{Zujovic2009,
abstract = {This paper describes an approach to automatically classify digital pictures of paintings by artistic genre. While the task of artistic classification is often entrusted to human experts, recent advances in machine learning and multimedia feature extraction has made this task easier to automate. Automatic classification is useful for organizing large digital collections, for automatic artistic recommendation, and even for mobile capture and identification by consumers. Our evaluation uses variable-resolution painting data gathered across Internet sources rather than solely using professional high-resolution data. Consequently, we believe this solution better addresses the task of classifying consumer-quality digital captures than other existing approaches. We include a comparison to existing feature extraction and classification methods as well as an analysis of our own approach across classifiers and feature vectors. {\textcopyright} 2009 IEEE.},
author = {Zujovic, Jana and Gandy, Lisa and Friedman, Scott and Pardo, Bryan and Pappas, Thrasyvoulos N.},
booktitle = {2009 IEEE International Workshop on Multimedia Signal Processing},
doi = {10.1109/MMSP.2009.5293271},
isbn = {978-1-4244-4463-2},
month = {oct},
pages = {1--5},
publisher = {IEEE},
title = {{Classifying paintings by artistic genre: An analysis of features {\&} classifiers}},
url = {http://ieeexplore.ieee.org/document/5293271/},
year = {2009}
}
@article{Su,
abstract = {Recent research has revealed that the output of Deep Neural Networks (DNN) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution (DE). It requires less adversarial information (a black-box attack) and can fool more types of networks due to the inherent features of DE. The results show that 67.97{\%} of the natural images in Kaggle CIFAR-10 test dataset and 16.04{\%} of the ImageNet (ILSVRC 2012) test images can be perturbed to at least one target class by modifying just one pixel with 74.03{\%} and 22.91{\%} confidence on average. We also show the same vulnerability on the original CIFAR-10 dataset. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks. Besides, we also illustrate an important application of DE (or broadly speaking, evolutionary computation) in the domain of adversarial machine learning: creating tools that can effectively generate low-cost adversarial attacks against neural networks for evaluating robustness.},
archivePrefix = {arXiv},
arxivId = {1710.08864},
author = {Su, Jiawei and Vargas, Danilo Vasconcellos and Kouichi, Sakurai},
doi = {10.1109/TEVC.2019.2890858},
eprint = {1710.08864},
file = {:C$\backslash$:/Users/rlhil/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Su, Vargas, Sakurai - Unknown - One Pixel Attack for Fooling Deep Neural Networks.pdf:pdf},
keywords = {Convolutional Neural Network,Image Recognition,Index Terms-Differential Evolution,Information Security},
month = {oct},
title = {{One pixel attack for fooling deep neural networks}},
url = {https://ieeexplore.ieee.org/abstract/document/8601309 http://arxiv.org/abs/1710.08864 http://dx.doi.org/10.1109/TEVC.2019.2890858},
year = {2017}
}
@misc{openDataDumps,
title = {{Open Library Data Dumps | Open Library}},
url = {https://openlibrary.org/developers/dumps},
urldate = {2020-04-10}
}
@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1512.03385},
file = {:C$\backslash$:/Users/rlhil/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - Unknown - Deep Residual Learning for Image Recognition.pdf:pdf},
month = {dec},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://image-net.org/challenges/LSVRC/2015/ http://arxiv.org/abs/1512.03385},
year = {2015}
}
@article{Willis2006,
abstract = {People often draw trait inferences from the facial appearance of other people. We investigated the minimal conditions under which people make such inferences. In five experiments, each focusing on a specific trait judgment, we manipulated the exposure time of unfamiliar faces. Judgments made after a 100-ms exposure correlated highly with judgments made in the absence of time constraints, suggesting that this exposure time was sufficient for participants to form an impression. In fact, for all judgments-attractiveness, likeability, trustworthiness, competence, and aggressiveness-increased exposure time did not significantly increase the correlations. When exposure time increased from 100 to 500 ms, participants' judgments became more negative, response times for judgments decreased, and confidence in judgments increased. When exposure time increased from 500 to 1,000 ms, trait judgments and response times did not change significantly (with one exception), but confidence increased for some of the judgments; this result suggests that additional time may simply boost confidence in judgments. However, increased exposure time led to more differentiated person impressions.},
author = {Willis, Janine and Todorov, Alexander},
doi = {10.1111/j.1467-9280.2006.01750.x},
issn = {0956-7976},
journal = {Psychological Science},
month = {jul},
number = {7},
pages = {592--598},
pmid = {16866745},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{First Impressions}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16866745 http://journals.sagepub.com/doi/10.1111/j.1467-9280.2006.01750.x},
volume = {17},
year = {2006}
}
@article{Zhu2019,
abstract = {Books have been widely used to share information and contribute to human knowledge. However, the quantitative use of books as a method of scholarly communication is relatively unexamined compared to journal articles and conference papers. This study uses the COCI dataset (a comprehensive open citation dataset provided by OpenCitations) to explore books' roles in scholarly communication. The COCI data we analyzed includes 445,826,118 citations from 46,534,705 bibliographic entities. By analyzing such a large amount of data, we provide a thorough, multifaceted understanding of books. Among the investigated factors are 1) temporal changes to book citations; 2) book citation distributions; 3) years to citation peak; 4) citation half-life; and 5) characteristics of the most-cited books. Results show that books have received less than 4{\%} of total citations, and have been cited mainly by journal articles. Moreover, 97.96{\%} of books have been cited fewer than ten times. Books take longer than other bibliographic materials to reach peak citation levels, yet are cited for the same duration as journal articles. Most-cited books tend to cover general (yet essential) topics, theories, and technological concepts in mathematics and statistics.},
archivePrefix = {arXiv},
arxivId = {1906.06039},
author = {Zhu, Yongjun and Yan, Erjia and Peroni, Silvio and Che, Chao},
doi = {10.1007/s11192-019-03311-9},
eprint = {1906.06039},
file = {:C$\backslash$:/Users/rlhil/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu et al. - 2019 - Nine Million Book Items and Eleven Million Citations A Study of Book-Based Scholarly Communication Using OpenCitatio.pdf:pdf},
month = {jun},
title = {{Nine Million Book Items and Eleven Million Citations: A Study of Book-Based Scholarly Communication Using OpenCitations}},
url = {http://arxiv.org/abs/1906.06039 http://dx.doi.org/10.1007/s11192-019-03311-9},
year = {2019}
}
@article{Shinahara2019,
abstract = {In this paper, we conduct a large-scale study of font statistics in book covers and online advertisements. Through the statistical study, we try to understand how graphic designers relate fonts and content genres and identify the relationship between font styles, colors, and genres. We propose an automatic approach to extract font information from graphic designs by applying a sequence of character detection, style classification, and clustering techniques to the graphic designs. The extracted font information is accumulated together with genre information, such as romance or business, for further trend analysis. Through our unique empirical study, we show that the collected font statistics reveal interesting trends in terms of how typographic design represents the impression and the atmosphere of the content genres.},
archivePrefix = {arXiv},
arxivId = {1906.10269},
author = {Shinahara, Yuto and Karamatsu, Takuro and Harada, Daisuke and Yamaguchi, Kota and Uchida, Seiichi},
eprint = {1906.10269},
file = {:C$\backslash$:/Users/rlhil/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shinahara et al. - 2019 - Serif or Sans Visual Font Analytics on Book Covers and Online Advertisements.pdf:pdf},
month = {jun},
title = {{Serif or Sans: Visual Font Analytics on Book Covers and Online Advertisements}},
url = {http://arxiv.org/abs/1906.10269},
year = {2019}
}
@article{erhan2009,
author = {Erhan, Dumitru and Bengio, Y and Courville, Aaron and Vincent, Pascal},
journal = {Technical Report, Univerist{\'{e}} de Montr{\'{e}}al},
title = {{Visualizing Higher-Layer Features of a Deep Network}},
year = {2009}
}
@misc{keras-vis,
title = {{keisen/tf-keras-vis: Neural network visualization toolkit for tf.keras}},
url = {https://github.com/keisen/tf-keras-vis},
urldate = {2020-04-10}
}
@article{Tzanetakis2002,
abstract = {Musical genres are categorical labels created by humans to characterize pieces of music. A musical genre is characterized by the common characteristics shared by its members. These characteristics typically are related to the instrumentation, rhythmic structure, and harmonic content of the music. Genre hierarchies are commonly used to structure the large collections of music available on the Web. Currently musical genre annotation is performed manually. Automatic musical genre classification can assist or replace the human user in this process and would be a valuable addition to music information retrieval systems. In addition, automatic musical genre classification provides a frame-work for developing and evaluating features for any type of content-based analysis of musical signals. In this paper, the automatic classification of audio signals into an hierarchy of musical genres is explored. More specifically, three feature sets for representing timbral texture, rhythmic content and pitch content are proposed. The performance and relative importance of the proposed features is investigated by training statistical pattern recognition classifiers using real-world audio collections. Both whole file and real-time frame-based classification schemes are described. Using the proposed feature sets, classification of 61{\%} for ten musical genres is achieved. This result is comparable to results reported for human musical genre classification.},
author = {Tzanetakis, George and Cook, Perry},
doi = {10.1109/TSA.2002.800560},
issn = {1063-6676},
journal = {IEEE Transactions on Speech and Audio Processing},
keywords = {Audio classification,Beat analysis,Feature extraction,Musical genre classification,Wavelets},
month = {jul},
number = {5},
pages = {293--302},
title = {{Musical genre classification of audio signals}},
url = {http://ieeexplore.ieee.org/document/1021072/},
volume = {10},
year = {2002}
}
@techreport{Nguyen,
abstract = {Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study [30] revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99{\%} confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neu-ral networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call "fooling images" (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.},
author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
file = {:C$\backslash$:/Users/rlhil/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen, Yosinski, Clune - Unknown - Deep Neural Networks are Easily Fooled High Confidence Predictions for Unrecognizable Images.pdf:pdf},
title = {{Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images}},
url = {http://evolvingai.org/fooling.}
}
@book{AmericanLibraryAssociation.CommitteeonLibraryTerminology.1971,
author = {{American Library Association. Committee on Library Terminology.} and Thompson, Elizabeth H. (Elizabeth Hardy)},
isbn = {9780838900000},
pages = {159},
publisher = {American Library Association},
title = {{A.L.A. glossary of library terms : with a selection of terms in related fields}},
year = {1971}
}
@misc{AdriasG,
author = {G, Adrius},
booktitle = {rockingbookcovers},
title = {{Book Cover Design Prices in 2019 - Rocking Book Covers}},
url = {https://www.rockingbookcovers.com/book-cover-design/book-cover-design-prices-2017/},
urldate = {2020-03-20}
}
@article{Ledig2016,
abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
archivePrefix = {arXiv},
arxivId = {1609.04802},
author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
eprint = {1609.04802},
file = {:C$\backslash$:/Users/rlhil/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ledig et al. - 2016 - Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network.pdf:pdf},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
month = {sep},
pages = {105--114},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network}},
url = {http://arxiv.org/abs/1609.04802},
volume = {2017-Janua},
year = {2016}
}
@techreport{Pratt1993,
abstract = {Previously, we have introduced the idea of neural network transfer, where learning on a target problem is sped up by using the weights obtained from a network trained for a related source task. Here, we present a new algorithm. called Discriminability-Based Transfer (DBT), which uses an information measure to estimate the utility of hyperplanes defined by source weights in the target network, and rescales transferred weight magnitudes accordingly. Several experiments demonstrate that target networks initialized via DBT learn significantly faster than networks initialized randomly.},
author = {Pratt, L Y},
file = {:C$\backslash$:/Users/rlhil/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pratt - Unknown - Discriminability-Based Transfer between Neural Networks.pdf:pdf},
title = {{Discriminability-Based Transfer between Neural Networks}},
year = {1993}
}
@techreport{Nguyena,
abstract = {Deep neural networks (DNNs) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classification problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right-similar to why we study the human brain-and will enable researchers to further improve DNNs. One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. One such method is called activation maximization (AM), which synthesizes an input (e.g. an image) that highly activates a neuron. Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network (DGN). The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real, (2) reveals the features learned by each neuron in an interpretable way, (3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images).},
author = {Nguyen, Anh and Dosovitskiy, Alexey and Yosinski, Jason and Brox, Thomas and Clune, Jeff},
file = {:C$\backslash$:/Users/rlhil/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen et al. - Unknown - Synthesizing the preferred inputs for neurons in neural networks via deep generator networks.pdf:pdf},
title = {{Synthesizing the preferred inputs for neurons in neural networks via deep generator networks}}
}
@article{Cun1989,
author = {{Le Cun}, Y. and Jackel, L.D. and Boser, B. and Denker, J.S. and Graf, H.P. and Guyon, I. and Henderson, D. and Howard, R.E. and Hubbard, W.},
doi = {10.1109/35.41400},
issn = {0163-6804},
journal = {IEEE Communications Magazine},
month = {nov},
number = {11},
pages = {41--46},
title = {{Handwritten digit recognition: applications of neural network chips and automatic learning}},
url = {http://ieeexplore.ieee.org/document/41400/},
volume = {27},
year = {1989}
}
@article{Ronneberger2015,
abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
archivePrefix = {arXiv},
arxivId = {1505.04597},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
doi = {10.1007/978-3-319-24574-4_28},
eprint = {1505.04597},
file = {:C$\backslash$:/Users/rlhil/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ronneberger, Fischer, Brox - 2015 - U-net Convolutional networks for biomedical image segmentation.pdf:pdf},
isbn = {9783319245737},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
month = {may},
pages = {234--241},
publisher = {Springer Verlag},
title = {{U-Net: Convolutional Networks for Biomedical Image Segmentation}},
url = {http://arxiv.org/abs/1505.04597},
volume = {9351},
year = {2015}
}
@misc{Google,
author = {Google},
title = {{Colaboratory – Google}},
url = {https://research.google.com/colaboratory/faq.html},
urldate = {2020-03-21}
}
@book{Marr:1982:VCI:1095712,
address = {New York, NY, USA},
author = {Marr, David},
isbn = {0716715678},
keywords = {computer vision},
publisher = {Henry Holt and Co., Inc.},
title = {{Vision: A Computational Investigation into the Human Representation and Processing of Visual Information}},
year = {1982}
}
@article{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {1409.1556},
file = {:C$\backslash$:/Users/rlhil/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan, Zisserman - 2015 - VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION.pdf:pdf},
keywords = {()},
month = {sep},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://www.robots.ox.ac.uk/ http://arxiv.org/abs/1409.1556},
year = {2014}
}
@article{Gideon2018,
abstract = {Handwritten signatures are very important in our social and legal life for verification and authentication. A signature can be accepted only if it is from the intended person. The probability of two signatures made by the same person being the same is very less. Many properties of the signature may vary even when two signatures are made by the same person. So, detecting a forgery becomes a challenging task. In this paper, a solution based on Convolutional Neural Network (CNN) is presented where the model is trained with a dataset of signatures, and predictions are made as to whether a provided signature is genuine or forged.},
author = {Gideon, S Jerome and Kandulna, Anurag and Kujur, Aron Abhishek and Diana, A. and Raimond, Kumudha},
doi = {10.1016/j.procs.2018.10.336},
file = {:C$\backslash$:/Users/rlhil/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jerome Gideon et al. - 2018 - Handwritten signature forgery detection using convolutional neural networks.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Convolutional neural networks,Deep learning,Machine learning,Signature forgery detection},
month = {jan},
pages = {978--987},
publisher = {Elsevier B.V.},
title = {{Handwritten Signature Forgery Detection using Convolutional Neural Networks}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1877050918320301},
volume = {143},
year = {2018}
}
@article{Everingham10,
author = {Everingham, M and Van{\~{}}Gool, L and Williams, C K I and Winn, J and Zisserman, A},
journal = {International Journal of Computer Vision},
month = {jun},
number = {2},
pages = {303--338},
title = {{The Pascal Visual Object Classes (VOC) Challenge}},
volume = {88},
year = {2010}
}
@article{Long2014,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
doi = {10.1109/TPAMI.2016.2572683},
eprint = {1411.4038},
file = {:C$\backslash$:/Users/rlhil/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Long, Shelhamer, Darrell - Unknown - Fully Convolutional Networks for Semantic Segmentation.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Convolutional Networks,Deep Learning,Semantic Segmentation,Transfer Learning},
month = {nov},
number = {4},
pages = {640--651},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
url = {http://arxiv.org/abs/1411.4038},
volume = {39},
year = {2014}
}
@article{Jolly2018,
abstract = {In this paper, we aim to understand the design principles in book cover images which are carefully crafted by experts. Book covers are designed in a unique way, specific to genres which convey important information to their readers. By using Convolutional Neural Networks (CNN) to predict book genres from cover images, visual cues which distinguish genres can be highlighted and analyzed. In order to understand these visual clues contributing towards the decision of a genre, we present the application of Layer-wise Relevance Propagation (LRP) on the book cover image classification results. We use LRP to explain the pixel-wise contributions of book cover design and highlight the design elements contributing towards particular genres. In addition, with the use of state-of-the-art object and text detection methods, insights about genre-specific book cover designs are discovered.},
archivePrefix = {arXiv},
arxivId = {1808.08402},
author = {Jolly, Shailza and Iwana, Brian Kenji and Kuroki, Ryohei and Uchida, Seiichi},
eprint = {1808.08402},
file = {:C$\backslash$:/Users/rlhil/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jolly et al. - 2018 - How do Convolutional Neural Networks Learn Design.pdf:pdf},
month = {aug},
title = {{How do Convolutional Neural Networks Learn Design?}},
url = {http://arxiv.org/abs/1808.08402},
year = {2018}
}
@misc{openAbout,
title = {{About Us | Open Library}},
url = {https://openlibrary.org/about},
urldate = {2020-04-10}
}
@article{Xiao2017,
abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
archivePrefix = {arXiv},
arxivId = {1708.07747},
author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
eprint = {1708.07747},
file = {:C$\backslash$:/Users/rlhil/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xiao, Rasul, Vollgraf - 2017 - Fashion-MNIST a Novel Image Dataset for Benchmarking Machine Learning Algorithms(2).pdf:pdf},
isbn = {1708.07747v2},
month = {aug},
title = {{Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms}},
url = {https://trends.google.com/trends/explore?date=all{\&}q=mnist,CIFAR,ImageNet http://arxiv.org/abs/1708.07747},
year = {2017}
}
@article{Szegedy,
abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
eprint = {1409.4842},
file = {:C$\backslash$:/Users/rlhil/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - Unknown - Going deeper with convolutions.pdf:pdf},
month = {sep},
title = {{Going Deeper with Convolutions}},
url = {http://arxiv.org/abs/1409.4842},
year = {2014}
}
@inproceedings{imagenet_cvpr09,
author = {Deng, J and Dong, W and Socher, R and Li, L.-J. and Li, K and Fei-Fei, L},
booktitle = {CVPR09},
title = {{ImageNet: A Large-Scale Hierarchical Image Database}},
year = {2009}
}
@misc{Watson,
author = {Watson, Amy},
booktitle = {statista.com},
title = {{Global book publishing revenue 2018-2023}},
url = {https://www.statista.com/statistics/307299/global-book-publishing-revenue/},
urldate = {2020-03-20},
year = {2019}
}
