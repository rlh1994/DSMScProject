{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1 Selecting Train and Test Data.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1qcuib9jUKlXZfHDH31CJ_3T8EsS-bR0X","authorship_tag":"ABX9TyPB9tfhVP7AV46GGSLi6myO"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"BuDuXb06h37H","colab_type":"text"},"source":["## Part 1: Load Libraries, load data and prep cuts of data\n","First we need to load all the data in, for the complete dataset and then the exisitng train and test sets. We then create sets of the keys from these data to identify what records have not been yet used in the training or test set as our remaining pool of records for later."]},{"cell_type":"code","metadata":{"id":"xT7lrrv-MHyM","colab_type":"code","colab":{}},"source":["# Import libraries and set workspace location\n","import numpy as np\n","import pandas as pd\n","import os\n","import shutil\n","import random\n","from matplotlib import pyplot as plt\n","from collections import Counter\n","from copy import deepcopy\n","from IPython import display\n","import time\n","%matplotlib inline\n","\n","os.chdir('/content/drive/My Drive')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nl17qOzBU0kd","colab_type":"code","colab":{}},"source":["# Function to take a record number and a dataframe and plot the image\n","def show_record(ASIN, df):\n","    cat = df[df['ASIN'] == str(ASIN)].iloc[0]['Category']\n","    plt.imshow(plt.imread(f'images/{cat}/{str(ASIN).zfill(10)}.jpg'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pprzvx3KPL_H","colab_type":"code","colab":{}},"source":["# Set the headers and read in the files\n","header_names = ['ASIN', 'Filename', 'Image URL', 'Title', 'Author', 'Category ID', 'Category']\n","\n","# All data\n","with open('Data/book32-listing.csv', mode='r', encoding='utf-8', errors='ignore') as f:\n","    all_images = pd.read_csv(f, delimiter=\",\", header=None, names=header_names)\n","\n","# Training data\n","with open('Data/book30-listing-train.csv', mode='r', encoding='utf-8', errors='ignore') as f:\n","    train = pd.read_csv(f, delimiter=\",\", header=None, names=header_names)\n","\n","# Test data\n","with open('Data/book30-listing-test.csv', mode='r', encoding='utf-8', errors='ignore') as f:\n","    test = pd.read_csv(f, delimiter=\",\", header=None, names=header_names)\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gkT-0KcoP-nC","colab_type":"code","colab":{}},"source":["# Create sets of ASINs in each group to be able to create new records\n","categories = train.Category.unique()\n","\n","all_asins = dict()\n","train_asins = dict()\n","test_asins = dict()\n","pos_asins = dict()\n","\n","for cat in categories:\n","    all_asins[cat] = set(all_images.query(f'Category == \"{cat}\"')['ASIN'])\n","    train_asins[cat] = set(train.query(f'Category == \"{cat}\"')['ASIN'])\n","    test_asins[cat] = set(test.query(f'Category == \"{cat}\"')['ASIN'])\n","\n","pos_asins = {cat : all_asins[cat].difference(train_asins[cat]).difference(test_asins[cat]) for cat in categories}\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LuwCfxVOXL5R","colab_type":"code","outputId":"5cf26771-9ef6-47c5-a9be-85279f0c9aef","executionInfo":{"status":"ok","timestamp":1588937863579,"user_tz":-60,"elapsed":453,"user":{"displayName":"Ryan Hill","photoUrl":"","userId":"12990220878877268494"}},"colab":{"base_uri":"https://localhost:8080/","height":527}},"source":["for k in pos_asins.keys():\n","    print(k, '\\t Starting Volume:', len(all_asins[k]), '\\t Remaining volume:', len(pos_asins[k]))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Biographies & Memoirs \t Starting Volume: 4261 \t Remaining volume: 2361\n","Children's Books \t Starting Volume: 13605 \t Remaining volume: 11705\n","Engineering & Transportation \t Starting Volume: 2672 \t Remaining volume: 772\n","Christian Books & Bibles \t Starting Volume: 9139 \t Remaining volume: 7239\n","Sports & Outdoors \t Starting Volume: 5968 \t Remaining volume: 4068\n","Health, Fitness & Dieting \t Starting Volume: 11886 \t Remaining volume: 9986\n","Medical Books \t Starting Volume: 12086 \t Remaining volume: 10186\n","Science & Math \t Starting Volume: 9276 \t Remaining volume: 7376\n","Travel \t Starting Volume: 18338 \t Remaining volume: 16438\n","Business & Money \t Starting Volume: 9965 \t Remaining volume: 8065\n","Cookbooks, Food & Wine \t Starting Volume: 8802 \t Remaining volume: 6902\n","Politics & Social Sciences \t Starting Volume: 3402 \t Remaining volume: 1502\n","Crafts, Hobbies & Home \t Starting Volume: 9934 \t Remaining volume: 8034\n","Religion & Spirituality \t Starting Volume: 7559 \t Remaining volume: 5659\n","Literature & Fiction \t Starting Volume: 7580 \t Remaining volume: 5680\n","Humor & Entertainment \t Starting Volume: 6896 \t Remaining volume: 4996\n","Law \t Starting Volume: 7314 \t Remaining volume: 5414\n","Computers & Technology \t Starting Volume: 7979 \t Remaining volume: 6079\n","Test Preparation \t Starting Volume: 2906 \t Remaining volume: 1006\n","Arts & Photography \t Starting Volume: 6460 \t Remaining volume: 4560\n","Parenting & Relationships \t Starting Volume: 2523 \t Remaining volume: 623\n","Romance \t Starting Volume: 4291 \t Remaining volume: 2391\n","History \t Starting Volume: 6807 \t Remaining volume: 4907\n","Comics & Graphic Novels \t Starting Volume: 3026 \t Remaining volume: 1126\n","Reference \t Starting Volume: 3268 \t Remaining volume: 1368\n","Teen & Young Adult \t Starting Volume: 7489 \t Remaining volume: 5589\n","Self-Help \t Starting Volume: 2703 \t Remaining volume: 803\n","Calendars \t Starting Volume: 2636 \t Remaining volume: 736\n","Science Fiction & Fantasy \t Starting Volume: 3800 \t Remaining volume: 1900\n","Mystery, Thriller & Suspense \t Starting Volume: 1998 \t Remaining volume: 98\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KU4DiDlOiNKn","colab_type":"text"},"source":["# Part 2: Identify possible box-set books\n","We want to remove any possible book covers that aren't \"front facing\" as is often the case with boxset offerings. These will need to be manually reviewed and we'll keep any set that has a single front facing image still, but side on images or side by sides will be removed."]},{"cell_type":"code","metadata":{"id":"gcAkphkXSR1r","colab_type":"code","outputId":"8bd1fc25-a3a6-4210-ad06-ba62dd17233f","executionInfo":{"status":"ok","timestamp":1588938197668,"user_tz":-60,"elapsed":867,"user":{"displayName":"Ryan Hill","photoUrl":"","userId":"12990220878877268494"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# Identify any potential record we might want to remove\n","train_issues = []\n","test_issues = []\n","\n","search_terms = ['(boxed set)', '(boxset)', '(box set)', '(anthology)', '(bundle)', '\\d(-book)', '\\d( book)']\n","\n","for term in search_terms:\n","    train_issues.extend(train[train['Title'].str.lower().str.contains(term, regex = True)]['ASIN'])\n","    test_issues.extend(test[test['Title'].str.lower().str.contains(term, regex = True)]['ASIN'])\n","\n","# Get unique records only\n","train_issues_clean = list(set(train_issues))\n","test_issues_clean = list(set(test_issues))\n","print('Potential number of issues: ', len(train_issues_clean) + len(test_issues_clean))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/strings.py:1954: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n","  return func(self, *args, **kwargs)\n"],"name":"stderr"},{"output_type":"stream","text":["Potential number of issues:  264\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6ZQFSfENZouA","colab_type":"code","colab":{}},"source":["# Move all files for manual review to it's own filter, and create a document to complete for the manual review\n","if os.path.exists('images/Manual Review'):\n","    shutil.rmtree('images/Manual Review')\n","\n","os.makedirs('images/Manual Review')\n","\n","for i, asin in zip(range(len(train_issues_clean)), train_issues_clean):\n","    print('Copying image', i)\n","    cat = all_images[all_images['ASIN'] == str(asin)].iloc[0]['Category']\n","    pth = f'images/{cat}/{asin.zfill(10)}.jpg'\n","    # google drive sorting doesn't work for some reason, so just going to manually number these up...\n","    shutil.copy(pth, f'images/Manual Review/train_{str(i).zfill(4)}.jpg')\n","\n","for i, asin in zip(range(len(test_issues_clean)), test_issues_clean):\n","    print('Copying image', i)\n","    cat = all_images[all_images['ASIN'] == str(asin)].iloc[0]['Category']\n","    pth = f'images/{cat}/{asin.zfill(10)}.jpg'\n","    # google drive sorting doesn't work for some reason, so just going to manually number these up...\n","    shutil.copy(pth, f'images/Manual Review/test_{str(i).zfill(4)}.jpg')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fK93o2W-bwKW","colab_type":"code","colab":{}},"source":["# Write a file with the new names to review\n","new_names_train = pd.DataFrame(zip(range(len(train_issues_clean)), train_issues_clean, ['Train']*len(train_issues_clean)), columns = ['new_filename', 'ASIN', 'Dataset'])\n","new_names_test = pd.DataFrame(zip(range(len(test_issues_clean)), test_issues_clean, ['Test']*len(test_issues_clean)), columns = ['new_filename', 'ASIN', 'Dataset'])\n","new_names = new_names_train.append(new_names_test)\n","data_to_write = all_images[all_images['ASIN'].isin(train_issues_clean + test_issues_clean)].merge(new_names, on = 'ASIN', how = 'left').sort_values(['Dataset', 'new_filename'])[['ASIN', 'Dataset', 'new_filename', 'Category', 'Title']]\n","data_to_write['Remove'] = 0\n","data_to_write.to_csv('Data/manual_data_file.csv', mode= 'w+')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C4kGf86ci5X_","colab_type":"text"},"source":["## Part 3: Load in review data and top up\n","Here we load back in the data that we manually reviewed, remove any records that were not front facing single images, and then use the remaining pool of records to replace these records with new ones. Importantly, we check these as well to ensure they don't fall into the same problem."]},{"cell_type":"code","metadata":{"id":"IZPHVhbPh2_Y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"23e2baf7-9144-4ae4-d370-c687207ccb95","executionInfo":{"status":"ok","timestamp":1588939828185,"user_tz":-60,"elapsed":406,"user":{"displayName":"Ryan Hill","photoUrl":"","userId":"12990220878877268494"}}},"source":["# Load data back in, filter to those that we want to remove\n","with open('Data/manual_data_file_completed.csv', mode='r', encoding='utf-8', errors='ignore') as f:\n","    reviewed_data = pd.read_csv(f, delimiter=\",\", header=0)\n","\n","reviewed_data = reviewed_data[reviewed_data['Remove'] == 1]\n","print(f'In total you are removing {len(reviewed_data)} records from your datasets.')"],"execution_count":20,"outputs":[{"output_type":"stream","text":["In total you are removing 94 records from your datasets.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OIt8fKmYjjcO","colab_type":"code","colab":{}},"source":["# count number to remove per category\n","num_issues_train = Counter(reviewed_data[reviewed_data['Dataset'] == 'Train']['Category'].tolist())\n","num_issues_test = Counter(reviewed_data[reviewed_data['Dataset'] == 'Test']['Category'].tolist())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nvjwpbCojue7","colab_type":"code","colab":{}},"source":["# set seed and sample this many new ones, check the names then change seed if needed...\n","pos_asins2 = deepcopy(pos_asins)\n","replacement_asins_train = []\n","for k, v in num_issues_train.items():\n","    random.seed(10) # go in loop due to possible different order of loop\n","    replacement_asins_train.extend(random.sample(pos_asins[k], v)) \n","    # make sure to remove before we do the test\n","    pos_asins2[k] = pos_asins[k].difference(set(replacement_asins_train))\n","\n","# remove those just selected before doing it for testing\n","replacement_asins_test = []\n","for k, v in num_issues_test.items():\n","    random.seed(10)\n","    replacement_asins_test.extend(random.sample(pos_asins2[k], v))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0P5M340gshdM","colab_type":"code","colab":{}},"source":["# Manually test these book covers as well, just do this locally as we expect it to mostly be fine\n","for i, record in zip(range(len(replacement_asins_train)), replacement_asins_train):\n","    print(i)\n","    show_record(record, all_images)\n","    display.clear_output(wait=True)\n","    display.display(plt.gcf())\n","    time.sleep(1.0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LJojin5yPBqw","colab_type":"code","colab":{}},"source":["for i, record in zip(range(len(replacement_asins_test)), replacement_asins_test):\n","    print(i)\n","    show_record(record, all_images)\n","    display.clear_output(wait=True)\n","    display.display(plt.gcf())\n","    time.sleep(1.0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qshu6lHkPxJ0","colab_type":"code","colab":{}},"source":["# Need to manually remove 3 records from the new train set\n","new_to_remove = [replacement_asins_train[0], replacement_asins_train[23], replacement_asins_train[26]]\n","\n","new_issues_train = Counter(all_images[all_images['ASIN'].isin(new_to_remove)]['Category'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6tKGThxiQWy2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"6e147013-316d-43ec-8e5d-edd5569481cf","executionInfo":{"status":"ok","timestamp":1588941158946,"user_tz":-60,"elapsed":577,"user":{"displayName":"Ryan Hill","photoUrl":"","userId":"12990220878877268494"}}},"source":["extra_replacement_asins_train = []\n","for k, v in new_issues_train.items():\n","    random.seed(12) # new seed otherwise we'd just get the same ones\n","    extra_replacement_asins_train.extend(random.sample(pos_asins[k], v)) \n","\n","# quick check we're not using them as extras already\n","print(f'Overlap with new train set: {len(set(extra_replacement_asins_train).intersection(set(replacement_asins_train)))}')\n","print(f'Overlap with new test set: {len(set(extra_replacement_asins_train).intersection(set(replacement_asins_test)))}')\n","\n","\n","print(f'New ASINs are: {extra_replacement_asins_train}')"],"execution_count":73,"outputs":[{"output_type":"stream","text":["Overlap with new train set: 0\n","Overlap with new test set: 0\n","New ASINs are: ['1449465749', '870335898', '1452501475']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b01pVBKfRR6Y","colab_type":"code","colab":{}},"source":["replacement_asins_train = list(set(replacement_asins_train).difference(set(new_to_remove))) + extra_replacement_asins_train\n","\n","replacement_images_train = all_images[all_images['ASIN'].isin(replacement_asins_train)]\n","replacement_images_test = all_images[all_images['ASIN'].isin(replacement_asins_test)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2M4cT62MjuhR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"9a7fbaf0-190e-414d-a194-a8de75a24c0a","executionInfo":{"status":"ok","timestamp":1588941405979,"user_tz":-60,"elapsed":428,"user":{"displayName":"Ryan Hill","photoUrl":"","userId":"12990220878877268494"}}},"source":["# remove from the training and test set and randomly replace with the new records\n","new_train = train[~train['ASIN'].isin(reviewed_data[reviewed_data['Dataset'] == 'Train']['ASIN'].tolist())].append(replacement_images_train)\n","\n","new_test = test[~test['ASIN'].isin(reviewed_data[reviewed_data['Dataset'] == 'Test']['ASIN'].tolist())].append(replacement_images_test)\n","\n","print(f'Number of records in new train: {len(new_train)}')\n","print(f'Number of records in new test: {len(new_test)}')"],"execution_count":83,"outputs":[{"output_type":"stream","text":["Number of records in new train: 51300\n","Number of records in new test: 5700\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zly8WYiSjulp","colab_type":"code","colab":{}},"source":["# save files and be done\n","new_train.to_csv('Data/new-book30-listing-train.csv', header = False)\n","new_train.to_csv('Data/new-bookcover30-labels-train.csv', header = False, columns = ['Filename', 'Category ID'])\n","\n","new_train.to_csv('Data/new-book30-listing-test.csv', header = False)\n","new_test.to_csv('Data/new-bookcover30-labels-test.csv', header = False, columns = ['Filename', 'Category ID'])"],"execution_count":0,"outputs":[]}]}