{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4 Train Core Models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG4a99INk9qa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install image-classifiers==1.0.0b1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwimBa7kinf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Library imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from sklearn.metrics import accuracy_score\n",
        "import re\n",
        "from classification_models.tfkeras import Classifiers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DHEqA8Jiv5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model/training variables. batch size must be divisor of number of training, validation, and test records.\n",
        "BATCH_SIZE = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSv5v_eqix31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://medium.com/@mrgarg.rajat/training-on-large-datasets-that-dont-fit-in-memory-in-keras-60a974785d71\n",
        "class My_Custom_Generator(tf.keras.utils.Sequence) :\n",
        "  \n",
        "  def __init__(self, image_filenames, labels, batch_size, IMG_HEIGHT, IMG_WIDTH) :\n",
        "    self.image_filenames = image_filenames\n",
        "    self.labels = labels\n",
        "    self.batch_size = batch_size\n",
        "    self.IMG_HEIGHT = IMG_HEIGHT\n",
        "    self.IMG_WIDTH = IMG_WIDTH\n",
        "    \n",
        "    \n",
        "  def __len__(self) :\n",
        "    return (np.ceil(len(self.image_filenames) / float(self.batch_size))).astype(np.int)\n",
        "  \n",
        "  \n",
        "  def __getitem__(self, idx) :\n",
        "    batch_x = self.image_filenames[idx * self.batch_size : (idx+1) * self.batch_size]\n",
        "    batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
        "    \n",
        "    return np.array([\n",
        "            np.resize(imread(str(file_name)), (self.IMG_HEIGHT, self.IMG_WIDTH, 3))\n",
        "               for file_name in batch_x])/255.0, np.array(batch_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5uK9dIiizqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data():\n",
        "    with open('/content/drive/My Drive/Data/final-book30-labels-train.csv', mode='r', encoding='utf-8', errors='ignore') as f:\n",
        "        train_labels = pd.read_csv(f, delimiter=\",\", header=None, names=['record', 'Filename', 'Category ID'])\n",
        "\n",
        "    with open('/content/drive/My Drive/Data/final-book30-labels-valid.csv', mode='r', encoding='utf-8', errors='ignore') as f:\n",
        "        valid_labels = pd.read_csv(f, delimiter=\",\", header=None, names=['record', 'Filename', 'Category ID'])\n",
        "\n",
        "    train_labels = train_labels.assign(Full_Filename = '/content/padded/'+ train_labels[\"Filename\"])\n",
        "    valid_labels = valid_labels.assign(Full_Filename = '/content/valid_padded/padded/'+ valid_labels[\"Filename\"])\n",
        "\n",
        "    print('Loading data')\n",
        "     # Load actual data\n",
        "    zip_path = '/content/drive/My Drive/images/Train/padded.zip'\n",
        "    !cp \"{zip_path}\" .\n",
        "    !unzip -q \"padded.zip\" \n",
        "    !rm \"padded.zip\" \n",
        "\n",
        "    zip_path = '/content/drive/My Drive/images/Valid/padded.zip'\n",
        "    !cp \"{zip_path}\" .\n",
        "    !unzip -q \"padded.zip\"  -d \"valid_padded\" \n",
        "    !rm \"padded.zip\" \n",
        "\n",
        "    return train_labels, valid_labels\n",
        "\n",
        "\n",
        "def train_model(base_model, IMG_HEIGHT, IMG_WIDTH, folder, train_labels, valid_labels):\n",
        "    \"\"\"\n",
        "    Function to load all the relevant data, train the model with early stopping, saving best validation accuracy snapshots, and \n",
        "        save training results to a file for later visualisation. \n",
        "    arg:\n",
        "        dataset - name of the dataset files to work from\n",
        "        skip_load - should the load from drive into colab be skipped\n",
        "    \"\"\"\n",
        "   \n",
        "    print('Prepping model')\n",
        "    my_training_batch_generator = My_Custom_Generator(train_labels[\"Full_Filename\"], train_labels[\"Category ID\"], BATCH_SIZE, IMG_HEIGHT, IMG_WIDTH)\n",
        "    my_validation_batch_generator = My_Custom_Generator(valid_labels[\"Full_Filename\"], valid_labels[\"Category ID\"], BATCH_SIZE, IMG_HEIGHT, IMG_WIDTH)\n",
        "\n",
        "\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(30, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='Adam',\n",
        "              loss='SparseCategoricalCrossentropy',\n",
        "              metrics=['accuracy', tf.keras.metrics.SparseTopKCategoricalAccuracy(3)])\n",
        "    \n",
        "      #early stopping and checkpoints\n",
        "    es = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', verbose=1, patience=20)\n",
        "    mc = tf.keras.callbacks.ModelCheckpoint(f'/content/drive/My Drive/Models/{folder}/' + '{epoch:02d}.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True, save_weights_only=True)\n",
        "    cl = tf.keras.callbacks.CSVLogger(f\"/content/drive/My Drive/Models/{folder}/model_history_log.csv\", append = True)\n",
        "\n",
        "    epoch_files = os.listdir(f'/content/drive/My Drive/Models/{folder}/')\n",
        "    pat = re.compile(f'^.*\\.h5$')\n",
        "    epoch_files_cut = [i for i in epoch_files if pat.match(i) ]\n",
        "    if len(epoch_files_cut) > 0:\n",
        "        model_weights = max(epoch_files_cut)\n",
        "        epoch = int(os.path.basename(model_weights)[:-3])\n",
        "\n",
        "        model.load_weights(f'/content/drive/My Drive/Models/{folder}/{model_weights}')\n",
        "    else:\n",
        "        epoch = 0\n",
        "\n",
        "    print('Training model')\n",
        "    history = model.fit_generator(generator=my_training_batch_generator, \n",
        "                              validation_data = my_validation_batch_generator,\n",
        "                              steps_per_epoch = int(len(train_labels) // BATCH_SIZE),\n",
        "                              validation_steps = int(len(valid_labels) // BATCH_SIZE),\n",
        "                              initial_epoch = epoch,\n",
        "                              epochs = 200,\n",
        "                              verbose = 1,\n",
        "                              callbacks = [es, mc, cl])\n",
        "    \n",
        "    return history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNZPBbOqqh_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r padded\n",
        "!rm -r valid_padded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAW1FJiXpOd9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12396336-207c-4c53-c972-b0ed5375d8e9"
      },
      "source": [
        "train_labels, valid_labels = load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wcOSKPuju2O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "82adb5db-f7b0-4ceb-ed64-c8eb72e777d6"
      },
      "source": [
        "## MobileNetV2\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3),\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "mob_hist = train_model(base_model, 224, 224, 'mobilenet', train_labels, valid_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prepping model\n",
            "Training model\n",
            "WARNING:tensorflow:From <ipython-input-12-846259388a86>:76: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 3.2132 - accuracy: 0.1213 - sparse_top_k_categorical_accuracy: 0.2613\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.14561, saving model to /content/drive/My Drive/Models/mobilenet/01.h5\n",
            "1539/1539 [==============================] - 205s 133ms/step - loss: 3.2132 - accuracy: 0.1213 - sparse_top_k_categorical_accuracy: 0.2613 - val_loss: 3.1072 - val_accuracy: 0.1456 - val_sparse_top_k_categorical_accuracy: 0.2984\n",
            "Epoch 2/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 3.0491 - accuracy: 0.1574 - sparse_top_k_categorical_accuracy: 0.3205\n",
            "Epoch 00002: val_accuracy improved from 0.14561 to 0.14795, saving model to /content/drive/My Drive/Models/mobilenet/02.h5\n",
            "1539/1539 [==============================] - 201s 130ms/step - loss: 3.0491 - accuracy: 0.1574 - sparse_top_k_categorical_accuracy: 0.3205 - val_loss: 3.0900 - val_accuracy: 0.1480 - val_sparse_top_k_categorical_accuracy: 0.3144\n",
            "Epoch 3/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.9757 - accuracy: 0.1757 - sparse_top_k_categorical_accuracy: 0.3459\n",
            "Epoch 00003: val_accuracy did not improve from 0.14795\n",
            "1539/1539 [==============================] - 201s 131ms/step - loss: 2.9757 - accuracy: 0.1757 - sparse_top_k_categorical_accuracy: 0.3459 - val_loss: 3.1948 - val_accuracy: 0.1382 - val_sparse_top_k_categorical_accuracy: 0.2862\n",
            "Epoch 4/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.9287 - accuracy: 0.1868 - sparse_top_k_categorical_accuracy: 0.3606\n",
            "Epoch 00004: val_accuracy improved from 0.14795 to 0.15984, saving model to /content/drive/My Drive/Models/mobilenet/04.h5\n",
            "1539/1539 [==============================] - 201s 131ms/step - loss: 2.9287 - accuracy: 0.1868 - sparse_top_k_categorical_accuracy: 0.3606 - val_loss: 3.1041 - val_accuracy: 0.1598 - val_sparse_top_k_categorical_accuracy: 0.3179\n",
            "Epoch 5/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.8949 - accuracy: 0.1938 - sparse_top_k_categorical_accuracy: 0.3733\n",
            "Epoch 00005: val_accuracy did not improve from 0.15984\n",
            "1539/1539 [==============================] - 200s 130ms/step - loss: 2.8949 - accuracy: 0.1938 - sparse_top_k_categorical_accuracy: 0.3733 - val_loss: 3.0928 - val_accuracy: 0.1577 - val_sparse_top_k_categorical_accuracy: 0.3084\n",
            "Epoch 6/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.8637 - accuracy: 0.2015 - sparse_top_k_categorical_accuracy: 0.3850\n",
            "Epoch 00006: val_accuracy did not improve from 0.15984\n",
            "1539/1539 [==============================] - 200s 130ms/step - loss: 2.8637 - accuracy: 0.2015 - sparse_top_k_categorical_accuracy: 0.3850 - val_loss: 3.1373 - val_accuracy: 0.1519 - val_sparse_top_k_categorical_accuracy: 0.3066\n",
            "Epoch 7/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.8398 - accuracy: 0.2077 - sparse_top_k_categorical_accuracy: 0.3920\n",
            "Epoch 00007: val_accuracy did not improve from 0.15984\n",
            "1539/1539 [==============================] - 199s 129ms/step - loss: 2.8398 - accuracy: 0.2077 - sparse_top_k_categorical_accuracy: 0.3920 - val_loss: 3.1459 - val_accuracy: 0.1528 - val_sparse_top_k_categorical_accuracy: 0.3092\n",
            "Epoch 8/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.8158 - accuracy: 0.2121 - sparse_top_k_categorical_accuracy: 0.3989\n",
            "Epoch 00008: val_accuracy did not improve from 0.15984\n",
            "1539/1539 [==============================] - 199s 129ms/step - loss: 2.8158 - accuracy: 0.2121 - sparse_top_k_categorical_accuracy: 0.3989 - val_loss: 3.1307 - val_accuracy: 0.1558 - val_sparse_top_k_categorical_accuracy: 0.3172\n",
            "Epoch 9/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.7982 - accuracy: 0.2157 - sparse_top_k_categorical_accuracy: 0.4068\n",
            "Epoch 00009: val_accuracy did not improve from 0.15984\n",
            "1539/1539 [==============================] - 203s 132ms/step - loss: 2.7982 - accuracy: 0.2157 - sparse_top_k_categorical_accuracy: 0.4068 - val_loss: 3.1155 - val_accuracy: 0.1573 - val_sparse_top_k_categorical_accuracy: 0.3090\n",
            "Epoch 10/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.7872 - accuracy: 0.2194 - sparse_top_k_categorical_accuracy: 0.4122\n",
            "Epoch 00010: val_accuracy improved from 0.15984 to 0.16842, saving model to /content/drive/My Drive/Models/mobilenet/10.h5\n",
            "1539/1539 [==============================] - 203s 132ms/step - loss: 2.7872 - accuracy: 0.2194 - sparse_top_k_categorical_accuracy: 0.4122 - val_loss: 3.1329 - val_accuracy: 0.1684 - val_sparse_top_k_categorical_accuracy: 0.3195\n",
            "Epoch 11/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.7671 - accuracy: 0.2248 - sparse_top_k_categorical_accuracy: 0.4188\n",
            "Epoch 00011: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 201s 131ms/step - loss: 2.7671 - accuracy: 0.2248 - sparse_top_k_categorical_accuracy: 0.4188 - val_loss: 3.1532 - val_accuracy: 0.1634 - val_sparse_top_k_categorical_accuracy: 0.3162\n",
            "Epoch 12/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.7660 - accuracy: 0.2255 - sparse_top_k_categorical_accuracy: 0.4181\n",
            "Epoch 00012: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 201s 131ms/step - loss: 2.7660 - accuracy: 0.2255 - sparse_top_k_categorical_accuracy: 0.4181 - val_loss: 3.1210 - val_accuracy: 0.1635 - val_sparse_top_k_categorical_accuracy: 0.3133\n",
            "Epoch 13/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.7439 - accuracy: 0.2286 - sparse_top_k_categorical_accuracy: 0.4241\n",
            "Epoch 00013: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 202s 131ms/step - loss: 2.7439 - accuracy: 0.2286 - sparse_top_k_categorical_accuracy: 0.4241 - val_loss: 3.1692 - val_accuracy: 0.1559 - val_sparse_top_k_categorical_accuracy: 0.3172\n",
            "Epoch 14/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.7352 - accuracy: 0.2301 - sparse_top_k_categorical_accuracy: 0.4315\n",
            "Epoch 00014: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 203s 132ms/step - loss: 2.7352 - accuracy: 0.2301 - sparse_top_k_categorical_accuracy: 0.4315 - val_loss: 3.1589 - val_accuracy: 0.1649 - val_sparse_top_k_categorical_accuracy: 0.3185\n",
            "Epoch 15/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.7307 - accuracy: 0.2337 - sparse_top_k_categorical_accuracy: 0.4278\n",
            "Epoch 00015: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 203s 132ms/step - loss: 2.7307 - accuracy: 0.2337 - sparse_top_k_categorical_accuracy: 0.4278 - val_loss: 3.1979 - val_accuracy: 0.1571 - val_sparse_top_k_categorical_accuracy: 0.3107\n",
            "Epoch 16/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.7204 - accuracy: 0.2373 - sparse_top_k_categorical_accuracy: 0.4338\n",
            "Epoch 00016: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 203s 132ms/step - loss: 2.7204 - accuracy: 0.2373 - sparse_top_k_categorical_accuracy: 0.4338 - val_loss: 3.1694 - val_accuracy: 0.1561 - val_sparse_top_k_categorical_accuracy: 0.3136\n",
            "Epoch 17/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.7069 - accuracy: 0.2392 - sparse_top_k_categorical_accuracy: 0.4382\n",
            "Epoch 00017: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 203s 132ms/step - loss: 2.7069 - accuracy: 0.2392 - sparse_top_k_categorical_accuracy: 0.4382 - val_loss: 3.1797 - val_accuracy: 0.1602 - val_sparse_top_k_categorical_accuracy: 0.3080\n",
            "Epoch 18/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.7036 - accuracy: 0.2386 - sparse_top_k_categorical_accuracy: 0.4367\n",
            "Epoch 00018: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 203s 132ms/step - loss: 2.7036 - accuracy: 0.2386 - sparse_top_k_categorical_accuracy: 0.4367 - val_loss: 3.1892 - val_accuracy: 0.1641 - val_sparse_top_k_categorical_accuracy: 0.3181\n",
            "Epoch 19/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6968 - accuracy: 0.2414 - sparse_top_k_categorical_accuracy: 0.4447\n",
            "Epoch 00019: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 203s 132ms/step - loss: 2.6968 - accuracy: 0.2414 - sparse_top_k_categorical_accuracy: 0.4447 - val_loss: 3.1843 - val_accuracy: 0.1639 - val_sparse_top_k_categorical_accuracy: 0.3246\n",
            "Epoch 20/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6951 - accuracy: 0.2418 - sparse_top_k_categorical_accuracy: 0.4436\n",
            "Epoch 00020: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 203s 132ms/step - loss: 2.6951 - accuracy: 0.2418 - sparse_top_k_categorical_accuracy: 0.4436 - val_loss: 3.2589 - val_accuracy: 0.1581 - val_sparse_top_k_categorical_accuracy: 0.3111\n",
            "Epoch 21/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6850 - accuracy: 0.2442 - sparse_top_k_categorical_accuracy: 0.4462\n",
            "Epoch 00021: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 203s 132ms/step - loss: 2.6850 - accuracy: 0.2442 - sparse_top_k_categorical_accuracy: 0.4462 - val_loss: 3.2090 - val_accuracy: 0.1513 - val_sparse_top_k_categorical_accuracy: 0.3094\n",
            "Epoch 22/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6869 - accuracy: 0.2433 - sparse_top_k_categorical_accuracy: 0.4453\n",
            "Epoch 00022: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 203s 132ms/step - loss: 2.6869 - accuracy: 0.2433 - sparse_top_k_categorical_accuracy: 0.4453 - val_loss: 3.4577 - val_accuracy: 0.1519 - val_sparse_top_k_categorical_accuracy: 0.2903\n",
            "Epoch 23/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6828 - accuracy: 0.2464 - sparse_top_k_categorical_accuracy: 0.4465\n",
            "Epoch 00023: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 203s 132ms/step - loss: 2.6828 - accuracy: 0.2464 - sparse_top_k_categorical_accuracy: 0.4465 - val_loss: 3.2902 - val_accuracy: 0.1468 - val_sparse_top_k_categorical_accuracy: 0.2953\n",
            "Epoch 24/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6781 - accuracy: 0.2481 - sparse_top_k_categorical_accuracy: 0.4481\n",
            "Epoch 00024: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 203s 132ms/step - loss: 2.6781 - accuracy: 0.2481 - sparse_top_k_categorical_accuracy: 0.4481 - val_loss: 3.2526 - val_accuracy: 0.1635 - val_sparse_top_k_categorical_accuracy: 0.3115\n",
            "Epoch 25/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6747 - accuracy: 0.2469 - sparse_top_k_categorical_accuracy: 0.4503\n",
            "Epoch 00025: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 202s 131ms/step - loss: 2.6747 - accuracy: 0.2469 - sparse_top_k_categorical_accuracy: 0.4503 - val_loss: 3.2627 - val_accuracy: 0.1556 - val_sparse_top_k_categorical_accuracy: 0.3076\n",
            "Epoch 26/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6688 - accuracy: 0.2478 - sparse_top_k_categorical_accuracy: 0.4523\n",
            "Epoch 00026: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 200s 130ms/step - loss: 2.6688 - accuracy: 0.2478 - sparse_top_k_categorical_accuracy: 0.4523 - val_loss: 3.2108 - val_accuracy: 0.1618 - val_sparse_top_k_categorical_accuracy: 0.3187\n",
            "Epoch 27/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6740 - accuracy: 0.2456 - sparse_top_k_categorical_accuracy: 0.4489\n",
            "Epoch 00027: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 199s 129ms/step - loss: 2.6740 - accuracy: 0.2456 - sparse_top_k_categorical_accuracy: 0.4489 - val_loss: 3.2601 - val_accuracy: 0.1536 - val_sparse_top_k_categorical_accuracy: 0.3078\n",
            "Epoch 28/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6635 - accuracy: 0.2492 - sparse_top_k_categorical_accuracy: 0.4531\n",
            "Epoch 00028: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 197s 128ms/step - loss: 2.6635 - accuracy: 0.2492 - sparse_top_k_categorical_accuracy: 0.4531 - val_loss: 3.2685 - val_accuracy: 0.1470 - val_sparse_top_k_categorical_accuracy: 0.3080\n",
            "Epoch 29/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6658 - accuracy: 0.2488 - sparse_top_k_categorical_accuracy: 0.4545\n",
            "Epoch 00029: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 197s 128ms/step - loss: 2.6658 - accuracy: 0.2488 - sparse_top_k_categorical_accuracy: 0.4545 - val_loss: 3.2869 - val_accuracy: 0.1556 - val_sparse_top_k_categorical_accuracy: 0.3051\n",
            "Epoch 30/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6555 - accuracy: 0.2550 - sparse_top_k_categorical_accuracy: 0.4567\n",
            "Epoch 00030: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 197s 128ms/step - loss: 2.6555 - accuracy: 0.2550 - sparse_top_k_categorical_accuracy: 0.4567 - val_loss: 3.2510 - val_accuracy: 0.1669 - val_sparse_top_k_categorical_accuracy: 0.3179\n",
            "Epoch 31/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6501 - accuracy: 0.2535 - sparse_top_k_categorical_accuracy: 0.4579\n",
            "Epoch 00031: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 197s 128ms/step - loss: 2.6501 - accuracy: 0.2535 - sparse_top_k_categorical_accuracy: 0.4579 - val_loss: 3.3084 - val_accuracy: 0.1577 - val_sparse_top_k_categorical_accuracy: 0.3016\n",
            "Epoch 32/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6471 - accuracy: 0.2537 - sparse_top_k_categorical_accuracy: 0.4578\n",
            "Epoch 00032: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 197s 128ms/step - loss: 2.6471 - accuracy: 0.2537 - sparse_top_k_categorical_accuracy: 0.4578 - val_loss: 3.2712 - val_accuracy: 0.1610 - val_sparse_top_k_categorical_accuracy: 0.3078\n",
            "Epoch 33/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6523 - accuracy: 0.2545 - sparse_top_k_categorical_accuracy: 0.4555\n",
            "Epoch 00033: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 196s 127ms/step - loss: 2.6523 - accuracy: 0.2545 - sparse_top_k_categorical_accuracy: 0.4555 - val_loss: 3.3037 - val_accuracy: 0.1532 - val_sparse_top_k_categorical_accuracy: 0.2990\n",
            "Epoch 34/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6479 - accuracy: 0.2546 - sparse_top_k_categorical_accuracy: 0.4590\n",
            "Epoch 00034: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 197s 128ms/step - loss: 2.6479 - accuracy: 0.2546 - sparse_top_k_categorical_accuracy: 0.4590 - val_loss: 3.2694 - val_accuracy: 0.1663 - val_sparse_top_k_categorical_accuracy: 0.3152\n",
            "Epoch 35/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6464 - accuracy: 0.2561 - sparse_top_k_categorical_accuracy: 0.4600\n",
            "Epoch 00035: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 197s 128ms/step - loss: 2.6464 - accuracy: 0.2561 - sparse_top_k_categorical_accuracy: 0.4600 - val_loss: 3.3339 - val_accuracy: 0.1483 - val_sparse_top_k_categorical_accuracy: 0.3055\n",
            "Epoch 36/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6451 - accuracy: 0.2522 - sparse_top_k_categorical_accuracy: 0.4604\n",
            "Epoch 00036: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 197s 128ms/step - loss: 2.6451 - accuracy: 0.2522 - sparse_top_k_categorical_accuracy: 0.4604 - val_loss: 3.3118 - val_accuracy: 0.1550 - val_sparse_top_k_categorical_accuracy: 0.2994\n",
            "Epoch 37/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6421 - accuracy: 0.2573 - sparse_top_k_categorical_accuracy: 0.4644\n",
            "Epoch 00037: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 197s 128ms/step - loss: 2.6421 - accuracy: 0.2573 - sparse_top_k_categorical_accuracy: 0.4644 - val_loss: 3.3114 - val_accuracy: 0.1575 - val_sparse_top_k_categorical_accuracy: 0.3070\n",
            "Epoch 38/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6388 - accuracy: 0.2589 - sparse_top_k_categorical_accuracy: 0.4626\n",
            "Epoch 00038: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 196s 128ms/step - loss: 2.6388 - accuracy: 0.2589 - sparse_top_k_categorical_accuracy: 0.4626 - val_loss: 3.3135 - val_accuracy: 0.1602 - val_sparse_top_k_categorical_accuracy: 0.3033\n",
            "Epoch 39/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6388 - accuracy: 0.2577 - sparse_top_k_categorical_accuracy: 0.4648\n",
            "Epoch 00039: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 196s 127ms/step - loss: 2.6388 - accuracy: 0.2577 - sparse_top_k_categorical_accuracy: 0.4648 - val_loss: 3.3316 - val_accuracy: 0.1534 - val_sparse_top_k_categorical_accuracy: 0.3006\n",
            "Epoch 40/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6338 - accuracy: 0.2580 - sparse_top_k_categorical_accuracy: 0.4637\n",
            "Epoch 00040: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 196s 128ms/step - loss: 2.6338 - accuracy: 0.2580 - sparse_top_k_categorical_accuracy: 0.4637 - val_loss: 3.3085 - val_accuracy: 0.1614 - val_sparse_top_k_categorical_accuracy: 0.3037\n",
            "Epoch 41/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6324 - accuracy: 0.2578 - sparse_top_k_categorical_accuracy: 0.4654\n",
            "Epoch 00041: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 196s 127ms/step - loss: 2.6324 - accuracy: 0.2578 - sparse_top_k_categorical_accuracy: 0.4654 - val_loss: 3.2906 - val_accuracy: 0.1637 - val_sparse_top_k_categorical_accuracy: 0.3105\n",
            "Epoch 42/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6368 - accuracy: 0.2575 - sparse_top_k_categorical_accuracy: 0.4617\n",
            "Epoch 00042: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 197s 128ms/step - loss: 2.6368 - accuracy: 0.2575 - sparse_top_k_categorical_accuracy: 0.4617 - val_loss: 3.3204 - val_accuracy: 0.1544 - val_sparse_top_k_categorical_accuracy: 0.2998\n",
            "Epoch 43/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6372 - accuracy: 0.2570 - sparse_top_k_categorical_accuracy: 0.4622\n",
            "Epoch 00043: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 198s 128ms/step - loss: 2.6372 - accuracy: 0.2570 - sparse_top_k_categorical_accuracy: 0.4622 - val_loss: 3.3897 - val_accuracy: 0.1446 - val_sparse_top_k_categorical_accuracy: 0.2932\n",
            "Epoch 44/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6331 - accuracy: 0.2586 - sparse_top_k_categorical_accuracy: 0.4654\n",
            "Epoch 00044: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 197s 128ms/step - loss: 2.6331 - accuracy: 0.2586 - sparse_top_k_categorical_accuracy: 0.4654 - val_loss: 3.4633 - val_accuracy: 0.1534 - val_sparse_top_k_categorical_accuracy: 0.3027\n",
            "Epoch 45/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6270 - accuracy: 0.2601 - sparse_top_k_categorical_accuracy: 0.4673\n",
            "Epoch 00045: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 200s 130ms/step - loss: 2.6270 - accuracy: 0.2601 - sparse_top_k_categorical_accuracy: 0.4673 - val_loss: 3.3716 - val_accuracy: 0.1472 - val_sparse_top_k_categorical_accuracy: 0.3014\n",
            "Epoch 46/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6247 - accuracy: 0.2583 - sparse_top_k_categorical_accuracy: 0.4660\n",
            "Epoch 00046: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 200s 130ms/step - loss: 2.6247 - accuracy: 0.2583 - sparse_top_k_categorical_accuracy: 0.4660 - val_loss: 3.3881 - val_accuracy: 0.1499 - val_sparse_top_k_categorical_accuracy: 0.2986\n",
            "Epoch 47/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6310 - accuracy: 0.2582 - sparse_top_k_categorical_accuracy: 0.4673\n",
            "Epoch 00047: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 201s 131ms/step - loss: 2.6310 - accuracy: 0.2582 - sparse_top_k_categorical_accuracy: 0.4673 - val_loss: 3.3654 - val_accuracy: 0.1550 - val_sparse_top_k_categorical_accuracy: 0.3039\n",
            "Epoch 48/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6257 - accuracy: 0.2611 - sparse_top_k_categorical_accuracy: 0.4669\n",
            "Epoch 00048: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 202s 131ms/step - loss: 2.6257 - accuracy: 0.2611 - sparse_top_k_categorical_accuracy: 0.4669 - val_loss: 3.3624 - val_accuracy: 0.1526 - val_sparse_top_k_categorical_accuracy: 0.3092\n",
            "Epoch 49/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6238 - accuracy: 0.2633 - sparse_top_k_categorical_accuracy: 0.4690\n",
            "Epoch 00049: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 202s 131ms/step - loss: 2.6238 - accuracy: 0.2633 - sparse_top_k_categorical_accuracy: 0.4690 - val_loss: 3.3634 - val_accuracy: 0.1532 - val_sparse_top_k_categorical_accuracy: 0.3033\n",
            "Epoch 50/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6215 - accuracy: 0.2591 - sparse_top_k_categorical_accuracy: 0.4694\n",
            "Epoch 00050: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 201s 130ms/step - loss: 2.6215 - accuracy: 0.2591 - sparse_top_k_categorical_accuracy: 0.4694 - val_loss: 3.3445 - val_accuracy: 0.1561 - val_sparse_top_k_categorical_accuracy: 0.3121\n",
            "Epoch 51/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6244 - accuracy: 0.2597 - sparse_top_k_categorical_accuracy: 0.4674\n",
            "Epoch 00051: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 199s 129ms/step - loss: 2.6244 - accuracy: 0.2597 - sparse_top_k_categorical_accuracy: 0.4674 - val_loss: 3.3436 - val_accuracy: 0.1532 - val_sparse_top_k_categorical_accuracy: 0.3047\n",
            "Epoch 52/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6217 - accuracy: 0.2613 - sparse_top_k_categorical_accuracy: 0.4706\n",
            "Epoch 00052: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 204s 132ms/step - loss: 2.6217 - accuracy: 0.2613 - sparse_top_k_categorical_accuracy: 0.4706 - val_loss: 3.3604 - val_accuracy: 0.1552 - val_sparse_top_k_categorical_accuracy: 0.3101\n",
            "Epoch 53/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6233 - accuracy: 0.2598 - sparse_top_k_categorical_accuracy: 0.4701\n",
            "Epoch 00053: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 203s 132ms/step - loss: 2.6233 - accuracy: 0.2598 - sparse_top_k_categorical_accuracy: 0.4701 - val_loss: 3.3717 - val_accuracy: 0.1589 - val_sparse_top_k_categorical_accuracy: 0.3107\n",
            "Epoch 54/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6199 - accuracy: 0.2616 - sparse_top_k_categorical_accuracy: 0.4671\n",
            "Epoch 00054: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 202s 131ms/step - loss: 2.6199 - accuracy: 0.2616 - sparse_top_k_categorical_accuracy: 0.4671 - val_loss: 3.4267 - val_accuracy: 0.1499 - val_sparse_top_k_categorical_accuracy: 0.3045\n",
            "Epoch 55/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6234 - accuracy: 0.2604 - sparse_top_k_categorical_accuracy: 0.4688\n",
            "Epoch 00055: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 203s 132ms/step - loss: 2.6234 - accuracy: 0.2604 - sparse_top_k_categorical_accuracy: 0.4688 - val_loss: 3.3782 - val_accuracy: 0.1573 - val_sparse_top_k_categorical_accuracy: 0.3033\n",
            "Epoch 56/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6173 - accuracy: 0.2622 - sparse_top_k_categorical_accuracy: 0.4712\n",
            "Epoch 00056: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 201s 131ms/step - loss: 2.6173 - accuracy: 0.2622 - sparse_top_k_categorical_accuracy: 0.4712 - val_loss: 3.3737 - val_accuracy: 0.1589 - val_sparse_top_k_categorical_accuracy: 0.3045\n",
            "Epoch 57/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6179 - accuracy: 0.2622 - sparse_top_k_categorical_accuracy: 0.4708\n",
            "Epoch 00057: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 200s 130ms/step - loss: 2.6179 - accuracy: 0.2622 - sparse_top_k_categorical_accuracy: 0.4708 - val_loss: 3.3804 - val_accuracy: 0.1559 - val_sparse_top_k_categorical_accuracy: 0.3133\n",
            "Epoch 58/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6199 - accuracy: 0.2602 - sparse_top_k_categorical_accuracy: 0.4717\n",
            "Epoch 00058: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 199s 129ms/step - loss: 2.6199 - accuracy: 0.2602 - sparse_top_k_categorical_accuracy: 0.4717 - val_loss: 3.4185 - val_accuracy: 0.1561 - val_sparse_top_k_categorical_accuracy: 0.3012\n",
            "Epoch 59/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6174 - accuracy: 0.2614 - sparse_top_k_categorical_accuracy: 0.4697\n",
            "Epoch 00059: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 201s 131ms/step - loss: 2.6174 - accuracy: 0.2614 - sparse_top_k_categorical_accuracy: 0.4697 - val_loss: 3.3816 - val_accuracy: 0.1495 - val_sparse_top_k_categorical_accuracy: 0.3012\n",
            "Epoch 60/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6123 - accuracy: 0.2653 - sparse_top_k_categorical_accuracy: 0.4724\n",
            "Epoch 00060: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 205s 133ms/step - loss: 2.6123 - accuracy: 0.2653 - sparse_top_k_categorical_accuracy: 0.4724 - val_loss: 3.3639 - val_accuracy: 0.1606 - val_sparse_top_k_categorical_accuracy: 0.3070\n",
            "Epoch 61/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6110 - accuracy: 0.2650 - sparse_top_k_categorical_accuracy: 0.4744\n",
            "Epoch 00061: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 203s 132ms/step - loss: 2.6110 - accuracy: 0.2650 - sparse_top_k_categorical_accuracy: 0.4744 - val_loss: 3.3709 - val_accuracy: 0.1608 - val_sparse_top_k_categorical_accuracy: 0.3158\n",
            "Epoch 62/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6179 - accuracy: 0.2632 - sparse_top_k_categorical_accuracy: 0.4734\n",
            "Epoch 00062: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 203s 132ms/step - loss: 2.6179 - accuracy: 0.2632 - sparse_top_k_categorical_accuracy: 0.4734 - val_loss: 3.4113 - val_accuracy: 0.1598 - val_sparse_top_k_categorical_accuracy: 0.3002\n",
            "Epoch 63/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6127 - accuracy: 0.2635 - sparse_top_k_categorical_accuracy: 0.4722\n",
            "Epoch 00063: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 205s 133ms/step - loss: 2.6127 - accuracy: 0.2635 - sparse_top_k_categorical_accuracy: 0.4722 - val_loss: 3.4073 - val_accuracy: 0.1565 - val_sparse_top_k_categorical_accuracy: 0.3096\n",
            "Epoch 64/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6158 - accuracy: 0.2630 - sparse_top_k_categorical_accuracy: 0.4717\n",
            "Epoch 00064: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 201s 130ms/step - loss: 2.6158 - accuracy: 0.2630 - sparse_top_k_categorical_accuracy: 0.4717 - val_loss: 3.4238 - val_accuracy: 0.1598 - val_sparse_top_k_categorical_accuracy: 0.3135\n",
            "Epoch 65/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6132 - accuracy: 0.2633 - sparse_top_k_categorical_accuracy: 0.4725\n",
            "Epoch 00065: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 201s 131ms/step - loss: 2.6132 - accuracy: 0.2633 - sparse_top_k_categorical_accuracy: 0.4725 - val_loss: 3.4521 - val_accuracy: 0.1581 - val_sparse_top_k_categorical_accuracy: 0.3047\n",
            "Epoch 66/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6093 - accuracy: 0.2656 - sparse_top_k_categorical_accuracy: 0.4719\n",
            "Epoch 00066: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 201s 130ms/step - loss: 2.6093 - accuracy: 0.2656 - sparse_top_k_categorical_accuracy: 0.4719 - val_loss: 3.4454 - val_accuracy: 0.1450 - val_sparse_top_k_categorical_accuracy: 0.2977\n",
            "Epoch 67/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6122 - accuracy: 0.2669 - sparse_top_k_categorical_accuracy: 0.4725\n",
            "Epoch 00067: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 202s 131ms/step - loss: 2.6122 - accuracy: 0.2669 - sparse_top_k_categorical_accuracy: 0.4725 - val_loss: 3.3904 - val_accuracy: 0.1579 - val_sparse_top_k_categorical_accuracy: 0.3025\n",
            "Epoch 68/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6130 - accuracy: 0.2663 - sparse_top_k_categorical_accuracy: 0.4746\n",
            "Epoch 00068: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 203s 132ms/step - loss: 2.6130 - accuracy: 0.2663 - sparse_top_k_categorical_accuracy: 0.4746 - val_loss: 3.5150 - val_accuracy: 0.1460 - val_sparse_top_k_categorical_accuracy: 0.2918\n",
            "Epoch 69/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6079 - accuracy: 0.2648 - sparse_top_k_categorical_accuracy: 0.4756\n",
            "Epoch 00069: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 203s 132ms/step - loss: 2.6079 - accuracy: 0.2648 - sparse_top_k_categorical_accuracy: 0.4756 - val_loss: 3.3974 - val_accuracy: 0.1530 - val_sparse_top_k_categorical_accuracy: 0.3023\n",
            "Epoch 70/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6103 - accuracy: 0.2653 - sparse_top_k_categorical_accuracy: 0.4723\n",
            "Epoch 00070: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 203s 132ms/step - loss: 2.6103 - accuracy: 0.2653 - sparse_top_k_categorical_accuracy: 0.4723 - val_loss: 3.4321 - val_accuracy: 0.1550 - val_sparse_top_k_categorical_accuracy: 0.3012\n",
            "Epoch 71/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6098 - accuracy: 0.2667 - sparse_top_k_categorical_accuracy: 0.4723\n",
            "Epoch 00071: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 203s 132ms/step - loss: 2.6098 - accuracy: 0.2667 - sparse_top_k_categorical_accuracy: 0.4723 - val_loss: 3.4041 - val_accuracy: 0.1639 - val_sparse_top_k_categorical_accuracy: 0.3099\n",
            "Epoch 72/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6043 - accuracy: 0.2668 - sparse_top_k_categorical_accuracy: 0.4751\n",
            "Epoch 00072: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 199s 129ms/step - loss: 2.6043 - accuracy: 0.2668 - sparse_top_k_categorical_accuracy: 0.4751 - val_loss: 3.4452 - val_accuracy: 0.1452 - val_sparse_top_k_categorical_accuracy: 0.3018\n",
            "Epoch 73/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6039 - accuracy: 0.2669 - sparse_top_k_categorical_accuracy: 0.4768\n",
            "Epoch 00073: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 204s 133ms/step - loss: 2.6039 - accuracy: 0.2669 - sparse_top_k_categorical_accuracy: 0.4768 - val_loss: 3.5308 - val_accuracy: 0.1372 - val_sparse_top_k_categorical_accuracy: 0.2926\n",
            "Epoch 74/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6107 - accuracy: 0.2634 - sparse_top_k_categorical_accuracy: 0.4757\n",
            "Epoch 00074: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 205s 133ms/step - loss: 2.6107 - accuracy: 0.2634 - sparse_top_k_categorical_accuracy: 0.4757 - val_loss: 3.4331 - val_accuracy: 0.1602 - val_sparse_top_k_categorical_accuracy: 0.3037\n",
            "Epoch 75/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6082 - accuracy: 0.2664 - sparse_top_k_categorical_accuracy: 0.4743\n",
            "Epoch 00075: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 205s 133ms/step - loss: 2.6082 - accuracy: 0.2664 - sparse_top_k_categorical_accuracy: 0.4743 - val_loss: 3.4296 - val_accuracy: 0.1458 - val_sparse_top_k_categorical_accuracy: 0.2981\n",
            "Epoch 76/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6062 - accuracy: 0.2652 - sparse_top_k_categorical_accuracy: 0.4764\n",
            "Epoch 00076: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 204s 133ms/step - loss: 2.6062 - accuracy: 0.2652 - sparse_top_k_categorical_accuracy: 0.4764 - val_loss: 3.4616 - val_accuracy: 0.1569 - val_sparse_top_k_categorical_accuracy: 0.2977\n",
            "Epoch 77/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6088 - accuracy: 0.2646 - sparse_top_k_categorical_accuracy: 0.4774\n",
            "Epoch 00077: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 204s 133ms/step - loss: 2.6088 - accuracy: 0.2646 - sparse_top_k_categorical_accuracy: 0.4774 - val_loss: 3.4477 - val_accuracy: 0.1542 - val_sparse_top_k_categorical_accuracy: 0.3082\n",
            "Epoch 78/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6135 - accuracy: 0.2644 - sparse_top_k_categorical_accuracy: 0.4724\n",
            "Epoch 00078: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 205s 133ms/step - loss: 2.6135 - accuracy: 0.2644 - sparse_top_k_categorical_accuracy: 0.4724 - val_loss: 3.4843 - val_accuracy: 0.1511 - val_sparse_top_k_categorical_accuracy: 0.3018\n",
            "Epoch 79/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6041 - accuracy: 0.2668 - sparse_top_k_categorical_accuracy: 0.4750\n",
            "Epoch 00079: val_accuracy did not improve from 0.16842\n",
            "1539/1539 [==============================] - 205s 133ms/step - loss: 2.6041 - accuracy: 0.2668 - sparse_top_k_categorical_accuracy: 0.4750 - val_loss: 3.4284 - val_accuracy: 0.1565 - val_sparse_top_k_categorical_accuracy: 0.3023\n",
            "Epoch 80/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.6087 - accuracy: 0.2638 - sparse_top_k_categorical_accuracy: 0.4750"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-fccdf2b94c7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                weights='imagenet')\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmob_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mobilenet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-846259388a86>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(base_model, IMG_HEIGHT, IMG_WIDTH, folder, train_labels, valid_labels)\u001b[0m\n\u001b[1;32m     74\u001b[0m                               \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                               \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                               callbacks = [es, mc, cl])\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1477\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m   @deprecation.deprecated(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    870\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m               return_dict=True)\n\u001b[0m\u001b[1;32m    873\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1079\u001b[0m                 step_num=step):\n\u001b[1;32m   1080\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay9hXdc1m0Qy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8bfeac1a-71f6-41f0-f800-9e70b9556394"
      },
      "source": [
        "## InceptionResnetV2\n",
        "base_model = tf.keras.applications.InceptionResNetV2(input_shape=(299, 299, 3),\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "inc_hist = train_model(base_model, 299, 299, 'inception_resnetv2', train_labels, valid_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prepping model\n",
            "Training model\n",
            "Epoch 47/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.2178 - accuracy: 0.3679 - sparse_top_k_categorical_accuracy: 0.5914\n",
            "Epoch 00047: val_accuracy improved from -inf to 0.27212, saving model to /content/drive/My Drive/Models/inception_resnetv2/47.h5\n",
            "1539/1539 [==============================] - 516s 336ms/step - loss: 2.2178 - accuracy: 0.3679 - sparse_top_k_categorical_accuracy: 0.5914 - val_loss: 2.7700 - val_accuracy: 0.2721 - val_sparse_top_k_categorical_accuracy: 0.4589\n",
            "Epoch 48/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.2197 - accuracy: 0.3658 - sparse_top_k_categorical_accuracy: 0.5924\n",
            "Epoch 00048: val_accuracy did not improve from 0.27212\n",
            "1539/1539 [==============================] - 511s 332ms/step - loss: 2.2197 - accuracy: 0.3658 - sparse_top_k_categorical_accuracy: 0.5924 - val_loss: 2.8252 - val_accuracy: 0.2618 - val_sparse_top_k_categorical_accuracy: 0.4468\n",
            "Epoch 49/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.2174 - accuracy: 0.3686 - sparse_top_k_categorical_accuracy: 0.5935\n",
            "Epoch 00049: val_accuracy did not improve from 0.27212\n",
            "1539/1539 [==============================] - 510s 332ms/step - loss: 2.2174 - accuracy: 0.3686 - sparse_top_k_categorical_accuracy: 0.5935 - val_loss: 2.7975 - val_accuracy: 0.2674 - val_sparse_top_k_categorical_accuracy: 0.4546\n",
            "Epoch 50/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.2094 - accuracy: 0.3682 - sparse_top_k_categorical_accuracy: 0.5959\n",
            "Epoch 00050: val_accuracy did not improve from 0.27212\n",
            "1539/1539 [==============================] - 509s 331ms/step - loss: 2.2094 - accuracy: 0.3682 - sparse_top_k_categorical_accuracy: 0.5959 - val_loss: 2.8023 - val_accuracy: 0.2665 - val_sparse_top_k_categorical_accuracy: 0.4554\n",
            "Epoch 51/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.2112 - accuracy: 0.3680 - sparse_top_k_categorical_accuracy: 0.5931\n",
            "Epoch 00051: val_accuracy did not improve from 0.27212\n",
            "1539/1539 [==============================] - 509s 331ms/step - loss: 2.2112 - accuracy: 0.3680 - sparse_top_k_categorical_accuracy: 0.5931 - val_loss: 2.8304 - val_accuracy: 0.2680 - val_sparse_top_k_categorical_accuracy: 0.4559\n",
            "Epoch 52/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.2071 - accuracy: 0.3704 - sparse_top_k_categorical_accuracy: 0.5975\n",
            "Epoch 00052: val_accuracy improved from 0.27212 to 0.27856, saving model to /content/drive/My Drive/Models/inception_resnetv2/52.h5\n",
            "1539/1539 [==============================] - 511s 332ms/step - loss: 2.2071 - accuracy: 0.3704 - sparse_top_k_categorical_accuracy: 0.5975 - val_loss: 2.7452 - val_accuracy: 0.2786 - val_sparse_top_k_categorical_accuracy: 0.4721\n",
            "Epoch 53/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.2080 - accuracy: 0.3682 - sparse_top_k_categorical_accuracy: 0.5945\n",
            "Epoch 00053: val_accuracy did not improve from 0.27856\n",
            "1539/1539 [==============================] - 510s 331ms/step - loss: 2.2080 - accuracy: 0.3682 - sparse_top_k_categorical_accuracy: 0.5945 - val_loss: 2.7929 - val_accuracy: 0.2782 - val_sparse_top_k_categorical_accuracy: 0.4581\n",
            "Epoch 54/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.2019 - accuracy: 0.3708 - sparse_top_k_categorical_accuracy: 0.5973\n",
            "Epoch 00054: val_accuracy improved from 0.27856 to 0.27914, saving model to /content/drive/My Drive/Models/inception_resnetv2/54.h5\n",
            "1539/1539 [==============================] - 512s 333ms/step - loss: 2.2019 - accuracy: 0.3708 - sparse_top_k_categorical_accuracy: 0.5973 - val_loss: 2.7888 - val_accuracy: 0.2791 - val_sparse_top_k_categorical_accuracy: 0.4593\n",
            "Epoch 55/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.1987 - accuracy: 0.3711 - sparse_top_k_categorical_accuracy: 0.5999\n",
            "Epoch 00055: val_accuracy did not improve from 0.27914\n",
            "1539/1539 [==============================] - 509s 331ms/step - loss: 2.1987 - accuracy: 0.3711 - sparse_top_k_categorical_accuracy: 0.5999 - val_loss: 2.7575 - val_accuracy: 0.2747 - val_sparse_top_k_categorical_accuracy: 0.4667\n",
            "Epoch 56/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.1959 - accuracy: 0.3725 - sparse_top_k_categorical_accuracy: 0.5989\n",
            "Epoch 00056: val_accuracy improved from 0.27914 to 0.28402, saving model to /content/drive/My Drive/Models/inception_resnetv2/56.h5\n",
            "1539/1539 [==============================] - 511s 332ms/step - loss: 2.1959 - accuracy: 0.3725 - sparse_top_k_categorical_accuracy: 0.5989 - val_loss: 2.7473 - val_accuracy: 0.2840 - val_sparse_top_k_categorical_accuracy: 0.4700\n",
            "Epoch 57/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.1965 - accuracy: 0.3718 - sparse_top_k_categorical_accuracy: 0.6005\n",
            "Epoch 00057: val_accuracy did not improve from 0.28402\n",
            "1539/1539 [==============================] - 509s 331ms/step - loss: 2.1965 - accuracy: 0.3718 - sparse_top_k_categorical_accuracy: 0.6005 - val_loss: 2.7765 - val_accuracy: 0.2725 - val_sparse_top_k_categorical_accuracy: 0.4657\n",
            "Epoch 58/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.2061 - accuracy: 0.3690 - sparse_top_k_categorical_accuracy: 0.5958\n",
            "Epoch 00058: val_accuracy did not improve from 0.28402\n",
            "1539/1539 [==============================] - 510s 331ms/step - loss: 2.2061 - accuracy: 0.3690 - sparse_top_k_categorical_accuracy: 0.5958 - val_loss: 2.7842 - val_accuracy: 0.2694 - val_sparse_top_k_categorical_accuracy: 0.4581\n",
            "Epoch 59/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.1879 - accuracy: 0.3720 - sparse_top_k_categorical_accuracy: 0.6044\n",
            "Epoch 00059: val_accuracy did not improve from 0.28402\n",
            "1539/1539 [==============================] - 510s 332ms/step - loss: 2.1879 - accuracy: 0.3720 - sparse_top_k_categorical_accuracy: 0.6044 - val_loss: 2.9069 - val_accuracy: 0.2612 - val_sparse_top_k_categorical_accuracy: 0.4456\n",
            "Epoch 60/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.1861 - accuracy: 0.3742 - sparse_top_k_categorical_accuracy: 0.6025\n",
            "Epoch 00060: val_accuracy did not improve from 0.28402\n",
            "1539/1539 [==============================] - 510s 332ms/step - loss: 2.1861 - accuracy: 0.3742 - sparse_top_k_categorical_accuracy: 0.6025 - val_loss: 2.7949 - val_accuracy: 0.2737 - val_sparse_top_k_categorical_accuracy: 0.4577\n",
            "Epoch 61/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.1828 - accuracy: 0.3738 - sparse_top_k_categorical_accuracy: 0.6032\n",
            "Epoch 00061: val_accuracy did not improve from 0.28402\n",
            "1539/1539 [==============================] - 510s 331ms/step - loss: 2.1828 - accuracy: 0.3738 - sparse_top_k_categorical_accuracy: 0.6032 - val_loss: 2.7680 - val_accuracy: 0.2782 - val_sparse_top_k_categorical_accuracy: 0.4682\n",
            "Epoch 62/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.1831 - accuracy: 0.3750 - sparse_top_k_categorical_accuracy: 0.6028\n",
            "Epoch 00062: val_accuracy did not improve from 0.28402\n",
            "1539/1539 [==============================] - 509s 331ms/step - loss: 2.1831 - accuracy: 0.3750 - sparse_top_k_categorical_accuracy: 0.6028 - val_loss: 2.7741 - val_accuracy: 0.2750 - val_sparse_top_k_categorical_accuracy: 0.4620\n",
            "Epoch 63/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.1812 - accuracy: 0.3770 - sparse_top_k_categorical_accuracy: 0.6032\n",
            "Epoch 00063: val_accuracy did not improve from 0.28402\n",
            "1539/1539 [==============================] - 509s 331ms/step - loss: 2.1812 - accuracy: 0.3770 - sparse_top_k_categorical_accuracy: 0.6032 - val_loss: 2.7665 - val_accuracy: 0.2813 - val_sparse_top_k_categorical_accuracy: 0.4618\n",
            "Epoch 64/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.1773 - accuracy: 0.3765 - sparse_top_k_categorical_accuracy: 0.6056\n",
            "Epoch 00064: val_accuracy did not improve from 0.28402\n",
            "1539/1539 [==============================] - 509s 331ms/step - loss: 2.1773 - accuracy: 0.3765 - sparse_top_k_categorical_accuracy: 0.6056 - val_loss: 2.7785 - val_accuracy: 0.2813 - val_sparse_top_k_categorical_accuracy: 0.4637\n",
            "Epoch 65/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.1755 - accuracy: 0.3774 - sparse_top_k_categorical_accuracy: 0.6042\n",
            "Epoch 00065: val_accuracy did not improve from 0.28402\n",
            "1539/1539 [==============================] - 509s 331ms/step - loss: 2.1755 - accuracy: 0.3774 - sparse_top_k_categorical_accuracy: 0.6042 - val_loss: 2.7926 - val_accuracy: 0.2780 - val_sparse_top_k_categorical_accuracy: 0.4610\n",
            "Epoch 66/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.1722 - accuracy: 0.3793 - sparse_top_k_categorical_accuracy: 0.6041\n",
            "Epoch 00066: val_accuracy did not improve from 0.28402\n",
            "1539/1539 [==============================] - 510s 331ms/step - loss: 2.1722 - accuracy: 0.3793 - sparse_top_k_categorical_accuracy: 0.6041 - val_loss: 2.8464 - val_accuracy: 0.2548 - val_sparse_top_k_categorical_accuracy: 0.4511\n",
            "Epoch 67/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.1740 - accuracy: 0.3791 - sparse_top_k_categorical_accuracy: 0.6065\n",
            "Epoch 00067: val_accuracy did not improve from 0.28402\n",
            "1539/1539 [==============================] - 510s 332ms/step - loss: 2.1740 - accuracy: 0.3791 - sparse_top_k_categorical_accuracy: 0.6065 - val_loss: 2.7823 - val_accuracy: 0.2811 - val_sparse_top_k_categorical_accuracy: 0.4630\n",
            "Epoch 68/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.1628 - accuracy: 0.3786 - sparse_top_k_categorical_accuracy: 0.6088\n",
            "Epoch 00068: val_accuracy did not improve from 0.28402\n",
            "1539/1539 [==============================] - 509s 330ms/step - loss: 2.1628 - accuracy: 0.3786 - sparse_top_k_categorical_accuracy: 0.6088 - val_loss: 2.7532 - val_accuracy: 0.2815 - val_sparse_top_k_categorical_accuracy: 0.4688\n",
            "Epoch 69/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.1571 - accuracy: 0.3812 - sparse_top_k_categorical_accuracy: 0.6102\n",
            "Epoch 00069: val_accuracy did not improve from 0.28402\n",
            "1539/1539 [==============================] - 511s 332ms/step - loss: 2.1571 - accuracy: 0.3812 - sparse_top_k_categorical_accuracy: 0.6102 - val_loss: 2.8587 - val_accuracy: 0.2626 - val_sparse_top_k_categorical_accuracy: 0.4507\n",
            "Epoch 70/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.1648 - accuracy: 0.3791 - sparse_top_k_categorical_accuracy: 0.6098\n",
            "Epoch 00070: val_accuracy did not improve from 0.28402\n",
            "1539/1539 [==============================] - 509s 331ms/step - loss: 2.1648 - accuracy: 0.3791 - sparse_top_k_categorical_accuracy: 0.6098 - val_loss: 2.8105 - val_accuracy: 0.2710 - val_sparse_top_k_categorical_accuracy: 0.4522\n",
            "Epoch 71/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.1590 - accuracy: 0.3820 - sparse_top_k_categorical_accuracy: 0.6097\n",
            "Epoch 00071: val_accuracy did not improve from 0.28402\n",
            "1539/1539 [==============================] - 511s 332ms/step - loss: 2.1590 - accuracy: 0.3820 - sparse_top_k_categorical_accuracy: 0.6097 - val_loss: 2.8037 - val_accuracy: 0.2828 - val_sparse_top_k_categorical_accuracy: 0.4682\n",
            "Epoch 72/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.1643 - accuracy: 0.3784 - sparse_top_k_categorical_accuracy: 0.6100\n",
            "Epoch 00072: val_accuracy did not improve from 0.28402\n",
            "1539/1539 [==============================] - 511s 332ms/step - loss: 2.1643 - accuracy: 0.3784 - sparse_top_k_categorical_accuracy: 0.6100 - val_loss: 2.8253 - val_accuracy: 0.2743 - val_sparse_top_k_categorical_accuracy: 0.4544\n",
            "Epoch 73/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.1636 - accuracy: 0.3797 - sparse_top_k_categorical_accuracy: 0.6086\n",
            "Epoch 00073: val_accuracy did not improve from 0.28402\n",
            "1539/1539 [==============================] - 511s 332ms/step - loss: 2.1636 - accuracy: 0.3797 - sparse_top_k_categorical_accuracy: 0.6086 - val_loss: 2.8187 - val_accuracy: 0.2698 - val_sparse_top_k_categorical_accuracy: 0.4608\n",
            "Epoch 74/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.1541 - accuracy: 0.3833 - sparse_top_k_categorical_accuracy: 0.6097\n",
            "Epoch 00074: val_accuracy did not improve from 0.28402\n",
            "1539/1539 [==============================] - 511s 332ms/step - loss: 2.1541 - accuracy: 0.3833 - sparse_top_k_categorical_accuracy: 0.6097 - val_loss: 2.8522 - val_accuracy: 0.2647 - val_sparse_top_k_categorical_accuracy: 0.4491\n",
            "Epoch 75/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.1601 - accuracy: 0.3787 - sparse_top_k_categorical_accuracy: 0.6087\n",
            "Epoch 00075: val_accuracy did not improve from 0.28402\n",
            "1539/1539 [==============================] - 511s 332ms/step - loss: 2.1601 - accuracy: 0.3787 - sparse_top_k_categorical_accuracy: 0.6087 - val_loss: 2.7865 - val_accuracy: 0.2766 - val_sparse_top_k_categorical_accuracy: 0.4655\n",
            "Epoch 76/1000\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 2.1563 - accuracy: 0.3813 - sparse_top_k_categorical_accuracy: 0.6118\n",
            "Epoch 00076: val_accuracy did not improve from 0.28402\n",
            "1539/1539 [==============================] - 511s 332ms/step - loss: 2.1563 - accuracy: 0.3813 - sparse_top_k_categorical_accuracy: 0.6118 - val_loss: 2.8772 - val_accuracy: 0.2712 - val_sparse_top_k_categorical_accuracy: 0.4540\n",
            "Epoch 00076: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J722B4yoj823",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "75eea60b-bbb2-44ae-a0f5-1edbbed05e83"
      },
      "source": [
        "# Resnext, has to come from anotehr package\n",
        "resnext50, preprocess_input = Classifiers.get('resnext50')\n",
        "base_model = resnext50((224, 224, 3), weights='imagenet', include_top = False)\n",
        "\n",
        "res_hist = train_model(base_model, 224, 224, 'resnext', train_labels, valid_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prepping model\n",
            "Training model\n",
            "Epoch 85/200\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 3.1520 - accuracy: 0.1258 - sparse_top_k_categorical_accuracy: 0.2725\n",
            "Epoch 00085: val_accuracy improved from -inf to 0.12417, saving model to /content/drive/My Drive/Models/resnext/85.h5\n",
            "1539/1539 [==============================] - 554s 360ms/step - loss: 3.1520 - accuracy: 0.1258 - sparse_top_k_categorical_accuracy: 0.2725 - val_loss: 3.2105 - val_accuracy: 0.1242 - val_sparse_top_k_categorical_accuracy: 0.2591\n",
            "Epoch 86/200\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 3.1473 - accuracy: 0.1293 - sparse_top_k_categorical_accuracy: 0.2718\n",
            "Epoch 00086: val_accuracy did not improve from 0.12417\n",
            "1539/1539 [==============================] - 545s 354ms/step - loss: 3.1473 - accuracy: 0.1293 - sparse_top_k_categorical_accuracy: 0.2718 - val_loss: 3.2091 - val_accuracy: 0.1238 - val_sparse_top_k_categorical_accuracy: 0.2624\n",
            "Epoch 87/200\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 3.1514 - accuracy: 0.1293 - sparse_top_k_categorical_accuracy: 0.2729\n",
            "Epoch 00087: val_accuracy did not improve from 0.12417\n",
            "1539/1539 [==============================] - 545s 354ms/step - loss: 3.1514 - accuracy: 0.1293 - sparse_top_k_categorical_accuracy: 0.2729 - val_loss: 3.1938 - val_accuracy: 0.1201 - val_sparse_top_k_categorical_accuracy: 0.2552\n",
            "Epoch 88/200\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 3.1493 - accuracy: 0.1281 - sparse_top_k_categorical_accuracy: 0.2741\n",
            "Epoch 00088: val_accuracy did not improve from 0.12417\n",
            "1539/1539 [==============================] - 546s 355ms/step - loss: 3.1493 - accuracy: 0.1281 - sparse_top_k_categorical_accuracy: 0.2741 - val_loss: 3.1881 - val_accuracy: 0.1240 - val_sparse_top_k_categorical_accuracy: 0.2573\n",
            "Epoch 89/200\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 3.1453 - accuracy: 0.1285 - sparse_top_k_categorical_accuracy: 0.2729\n",
            "Epoch 00089: val_accuracy did not improve from 0.12417\n",
            "1539/1539 [==============================] - 549s 357ms/step - loss: 3.1453 - accuracy: 0.1285 - sparse_top_k_categorical_accuracy: 0.2729 - val_loss: 3.1792 - val_accuracy: 0.1234 - val_sparse_top_k_categorical_accuracy: 0.2554\n",
            "Epoch 90/200\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 3.1449 - accuracy: 0.1302 - sparse_top_k_categorical_accuracy: 0.2733\n",
            "Epoch 00090: val_accuracy did not improve from 0.12417\n",
            "1539/1539 [==============================] - 553s 359ms/step - loss: 3.1449 - accuracy: 0.1302 - sparse_top_k_categorical_accuracy: 0.2733 - val_loss: 3.1772 - val_accuracy: 0.1214 - val_sparse_top_k_categorical_accuracy: 0.2673\n",
            "Epoch 91/200\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 3.1466 - accuracy: 0.1300 - sparse_top_k_categorical_accuracy: 0.2719\n",
            "Epoch 00091: val_accuracy did not improve from 0.12417\n",
            "1539/1539 [==============================] - 560s 364ms/step - loss: 3.1466 - accuracy: 0.1300 - sparse_top_k_categorical_accuracy: 0.2719 - val_loss: 3.2274 - val_accuracy: 0.1146 - val_sparse_top_k_categorical_accuracy: 0.2540\n",
            "Epoch 92/200\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 3.1438 - accuracy: 0.1296 - sparse_top_k_categorical_accuracy: 0.2753\n",
            "Epoch 00092: val_accuracy improved from 0.12417 to 0.12807, saving model to /content/drive/My Drive/Models/resnext/92.h5\n",
            "1539/1539 [==============================] - 586s 381ms/step - loss: 3.1438 - accuracy: 0.1296 - sparse_top_k_categorical_accuracy: 0.2753 - val_loss: 3.2153 - val_accuracy: 0.1281 - val_sparse_top_k_categorical_accuracy: 0.2587\n",
            "Epoch 93/200\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 3.1440 - accuracy: 0.1278 - sparse_top_k_categorical_accuracy: 0.2734\n",
            "Epoch 00093: val_accuracy did not improve from 0.12807\n",
            "1539/1539 [==============================] - 550s 357ms/step - loss: 3.1440 - accuracy: 0.1278 - sparse_top_k_categorical_accuracy: 0.2734 - val_loss: 3.1776 - val_accuracy: 0.1240 - val_sparse_top_k_categorical_accuracy: 0.2563\n",
            "Epoch 94/200\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 3.1421 - accuracy: 0.1295 - sparse_top_k_categorical_accuracy: 0.2763\n",
            "Epoch 00094: val_accuracy did not improve from 0.12807\n",
            "1539/1539 [==============================] - 549s 357ms/step - loss: 3.1421 - accuracy: 0.1295 - sparse_top_k_categorical_accuracy: 0.2763 - val_loss: 3.1850 - val_accuracy: 0.1189 - val_sparse_top_k_categorical_accuracy: 0.2542\n",
            "Epoch 95/200\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 3.1425 - accuracy: 0.1303 - sparse_top_k_categorical_accuracy: 0.2790\n",
            "Epoch 00095: val_accuracy improved from 0.12807 to 0.13119, saving model to /content/drive/My Drive/Models/resnext/95.h5\n",
            "1539/1539 [==============================] - 551s 358ms/step - loss: 3.1425 - accuracy: 0.1303 - sparse_top_k_categorical_accuracy: 0.2790 - val_loss: 3.1450 - val_accuracy: 0.1312 - val_sparse_top_k_categorical_accuracy: 0.2747\n",
            "Epoch 96/200\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 3.1372 - accuracy: 0.1299 - sparse_top_k_categorical_accuracy: 0.2778\n",
            "Epoch 00096: val_accuracy did not improve from 0.13119\n",
            "1539/1539 [==============================] - 548s 356ms/step - loss: 3.1372 - accuracy: 0.1299 - sparse_top_k_categorical_accuracy: 0.2778 - val_loss: 3.2654 - val_accuracy: 0.1142 - val_sparse_top_k_categorical_accuracy: 0.2376\n",
            "Epoch 97/200\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 3.1406 - accuracy: 0.1288 - sparse_top_k_categorical_accuracy: 0.2761\n",
            "Epoch 00097: val_accuracy did not improve from 0.13119\n",
            "1539/1539 [==============================] - 549s 357ms/step - loss: 3.1406 - accuracy: 0.1288 - sparse_top_k_categorical_accuracy: 0.2761 - val_loss: 3.2279 - val_accuracy: 0.1212 - val_sparse_top_k_categorical_accuracy: 0.2536\n",
            "Epoch 98/200\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 3.1392 - accuracy: 0.1294 - sparse_top_k_categorical_accuracy: 0.2762\n",
            "Epoch 00098: val_accuracy did not improve from 0.13119\n",
            "1539/1539 [==============================] - 549s 357ms/step - loss: 3.1392 - accuracy: 0.1294 - sparse_top_k_categorical_accuracy: 0.2762 - val_loss: 3.1677 - val_accuracy: 0.1232 - val_sparse_top_k_categorical_accuracy: 0.2634\n",
            "Epoch 99/200\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 3.1356 - accuracy: 0.1315 - sparse_top_k_categorical_accuracy: 0.2778\n",
            "Epoch 00099: val_accuracy improved from 0.13119 to 0.13158, saving model to /content/drive/My Drive/Models/resnext/99.h5\n",
            "1539/1539 [==============================] - 550s 357ms/step - loss: 3.1356 - accuracy: 0.1315 - sparse_top_k_categorical_accuracy: 0.2778 - val_loss: 3.1610 - val_accuracy: 0.1316 - val_sparse_top_k_categorical_accuracy: 0.2618\n",
            "Epoch 100/200\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 3.1391 - accuracy: 0.1308 - sparse_top_k_categorical_accuracy: 0.2792\n",
            "Epoch 00100: val_accuracy did not improve from 0.13158\n",
            "1539/1539 [==============================] - 549s 357ms/step - loss: 3.1391 - accuracy: 0.1308 - sparse_top_k_categorical_accuracy: 0.2792 - val_loss: 3.1827 - val_accuracy: 0.1281 - val_sparse_top_k_categorical_accuracy: 0.2548\n",
            "Epoch 101/200\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 3.1394 - accuracy: 0.1294 - sparse_top_k_categorical_accuracy: 0.2738\n",
            "Epoch 00101: val_accuracy did not improve from 0.13158\n",
            "1539/1539 [==============================] - 550s 357ms/step - loss: 3.1394 - accuracy: 0.1294 - sparse_top_k_categorical_accuracy: 0.2738 - val_loss: 3.1713 - val_accuracy: 0.1275 - val_sparse_top_k_categorical_accuracy: 0.2729\n",
            "Epoch 102/200\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 3.1376 - accuracy: 0.1326 - sparse_top_k_categorical_accuracy: 0.2781\n",
            "Epoch 00102: val_accuracy did not improve from 0.13158\n",
            "1539/1539 [==============================] - 551s 358ms/step - loss: 3.1376 - accuracy: 0.1326 - sparse_top_k_categorical_accuracy: 0.2781 - val_loss: 3.1860 - val_accuracy: 0.1164 - val_sparse_top_k_categorical_accuracy: 0.2517\n",
            "Epoch 103/200\n",
            "1539/1539 [==============================] - ETA: 0s - loss: 3.1328 - accuracy: 0.1323 - sparse_top_k_categorical_accuracy: 0.2796\n",
            "Epoch 00103: val_accuracy did not improve from 0.13158\n",
            "1539/1539 [==============================] - 550s 358ms/step - loss: 3.1328 - accuracy: 0.1323 - sparse_top_k_categorical_accuracy: 0.2796 - val_loss: 3.1891 - val_accuracy: 0.1166 - val_sparse_top_k_categorical_accuracy: 0.2507\n",
            "Epoch 104/200\n",
            "  17/1539 [..............................] - ETA: 7:40 - loss: 3.1166 - accuracy: 0.1373 - sparse_top_k_categorical_accuracy: 0.2588"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-0067471ae9ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnext50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_top\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mres_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'resnext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-24cab5561195>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(base_model, IMG_HEIGHT, IMG_WIDTH, folder, train_labels, valid_labels)\u001b[0m\n\u001b[1;32m     74\u001b[0m                               \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                               \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                               callbacks = [es, mc, cl])\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1477\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m   @deprecation.deprecated(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}